# test_lora_inference.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel


# âœ… ì—¬ê¸°ë¥¼ ë„¤ê°€ ì‹¤ì œë¡œ ì“´ ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë§ì¶°ì¤˜!
BASE_MODEL = "Qwen/Qwen2.5-7B-Instruct"   # ì˜ˆì‹œ
LORA_ADAPTER_DIR = "./manufacturing_lora_output"  # train_lora.pyì—ì„œ ì €ì¥í•œ ê²½ë¡œ


def load_model():
    print("ğŸ”¹ Base ëª¨ë¸ ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",          # A100ì´ë©´ ìë™ìœ¼ë¡œ GPU í• ë‹¹
        torch_dtype=torch.float16,  # VRAM ì ˆì•½
    )

    print("ğŸ”¹ LoRA ì–´ëŒ‘í„° ì ìš© ì¤‘...")
    model = PeftModel.from_pretrained(
        base_model,
        LORA_ADAPTER_DIR,
    )

    model.eval()
    return tokenizer, model


def run_inference(tokenizer, model):
    # ğŸ”§ í…ŒìŠ¤íŠ¸ìš© ì…ë ¥ (ì›í•˜ë©´ ì—¬ê¸°ë§Œ ë°”ê¿”ì„œ ê³„ì† ì‹œí—˜í•´ ë³´ë©´ ë¨)
    instruction = (
        "ë‹¹ì‹ ì€ ì œì¡° í˜„ì¥ì˜ ì „ë¬¸ ì„¤ë¹„ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. "
        "ì•„ë˜ ê³µì • ì´ìƒ ìƒí™©ì— ëŒ€í•´ ì›ì¸ì„ ë¶„ì„í•˜ê³ , ì²´í¬ë¦¬ìŠ¤íŠ¸ì™€ ì¡°ì¹˜ ê°€ì´ë“œ, "
        "8D Report ì´ˆì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”."
        "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ í˜•ì‹ì„ **ì—„ê²©íˆ ì§€ì¼œì„œ** ë‹µë³€í•˜ì„¸ìš”."

        "1) ğŸ§  ìƒí™© ë¶„ì„ ë° ì¶”ë¡  ê³¼ì •"
        "2) âœ… ì›ì¸ ë¶„ì„ ê²°ê³¼ (ìš°ì„ ìˆœìœ„ 3ê°œ)"
        "3) ğŸ“ ìš°ì„  ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸"
        "4) ğŸ”§ ë‹¨ê³„ë³„ ì¡°ì¹˜ ê°€ì´ë“œ (1ì°¨/2ì°¨/3ì°¨)"
        "5) ğŸ“‹ 8D Report ì´ˆì•ˆ (D1~D7 í•­ëª© í¬í•¨)"

    )

    input_text = f"""{instruction}

[ê³µì • ì´ìƒ ì´ë²¤íŠ¸]
ì„¤ë¹„: ì‚¬ì¶œê¸°-2í˜¸ê¸°
ë°œìƒì‹œê°: 2024-09-12 14:32:10
ì´ìƒìœ í˜•: ì˜¨ë„ ì´ìƒ

[ì„¼ì„œ ë°ì´í„°]
- ì‹¤ë¦°ë” ì˜¨ë„: 245Â°C (ì •ìƒ: 200Â°C)
- ì„ê³„ê°’: ì„¤ì •ê°’ ëŒ€ë¹„ Â±15Â°C
- ì¦ìƒ: ì œí’ˆ ë³€í˜•, ì¹˜ìˆ˜ ë¶ˆëŸ‰

[RAG ê²€ìƒ‰ ê²°ê³¼]
[ê³¼ê±° ì´ë ¥ #2023-08-15] ë™ì¼ ì¦ìƒìœ¼ë¡œ ëƒ‰ê° ì‹œìŠ¤í…œ ë§‰í˜ í™•ì¸ë¨. í•´ë‹¹ ë¶€í’ˆ êµì²´ í›„ ì •ìƒí™”.
[ì„¤ë¹„ ë§¤ë‰´ì–¼ 3.2ì ˆ] ì˜¨ë„ ì´ìƒ ë°œìƒ ì‹œ ëƒ‰ê° ì‹œìŠ¤í…œ ë§‰í˜ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ìŒ. ì¦‰ì‹œ ì ê²€ ê¶Œì¥.

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.
"""

    print("\nğŸ§¾ ===== ì…ë ¥ í”„ë¡¬í”„íŠ¸ =====\n")
    print(input_text)

    inputs = tokenizer(
        input_text,
        return_tensors="pt"
    ).to(model.device)

    print("\nğŸ¤– ì¶”ë¡  ì¤‘...\n")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=800,   # ì¶œë ¥ ê¸¸ì´
            temperature=0.3,      # ëœë¤ì„±(ë‚®ì„ìˆ˜ë¡ ë³´ìˆ˜ì )
            top_p=0.9,
            do_sample=True,
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print("\nğŸ“„ ===== ëª¨ë¸ ì¶œë ¥ =====\n")
    print(decoded)


def main():
    tokenizer, model = load_model()
    run_inference(tokenizer, model)


if __name__ == "__main__":
    main()
