#!/usr/bin/env python3
"""
ìµœê³  ì„±ëŠ¥ ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ (2ê°œ ëª¨ë¸ ì•™ìƒë¸”)

TimesNet + AnomalyTransformer ì‚¬ìš©
(TranADëŠ” PyTorch í˜¸í™˜ì„± ë¬¸ì œë¡œ ì œì™¸)

ì‚¬ìš©ë²•:
  python train_best_2models.py
"""

import os
import sys
import numpy as np
import pickle
from datetime import datetime
from pathlib import Path

project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))

print("="*80)
print("ğŸ¯ ìµœê³  ì„±ëŠ¥ ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ (2ê°œ ëª¨ë¸)")
print("="*80)
print("\nâš™ï¸  ì„¤ì •:")
print("   - ëª¨ë¸: TimesNet + AnomalyTransformer (ì•™ìƒë¸”)")
print("   - ì‹œí€€ìŠ¤ ê¸¸ì´: 50")
print("   - ì—í¬í¬: 50")
print("   - ì •ê·œí™”: í™œì„±í™”")
print("\nâ³ ì˜ˆìƒ ì‹œê°„: 40~60ë¶„")
print("ğŸ¯ ì˜ˆìƒ ì„±ëŠ¥: Recall 85~90%, Precision 88~93%")
print("\n" + "="*80)

# GPU í™•ì¸
try:
    import torch
    if torch.cuda.is_available():
        device = 'cuda'
        gpu_name = torch.cuda.get_device_name(0)
        print(f"\nâœ… GPU ì‚¬ìš©: {gpu_name}")
    else:
        device = 'cpu'
        print(f"\nâš ï¸  CPU ëª¨ë“œ")
except:
    device = 'cpu'

# ë°ì´í„° ë¡œë“œ
print(f"\n{'='*80}")
print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
print(f"{'='*80}")

import pandas as pd
from sklearn.preprocessing import StandardScaler

normal_path = str(project_root / "dataset_3" / "press_data_normal.csv")
outlier_path = str(project_root / "dataset_3" / "outlier_data.csv")

df_normal = pd.read_csv(normal_path)
df_outlier = pd.read_csv(outlier_path)

sensor_cols = ['AI0_Vibration', 'AI1_Vibration', 'AI2_Current']
normal_data = df_normal[sensor_cols].values.astype(np.float32)
outlier_data = df_outlier[sensor_cols].values.astype(np.float32)

print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
print(f"   ì •ìƒ: {len(normal_data):,}ê°œ")
print(f"   ì´ìƒ: {len(outlier_data):,}ê°œ")

# ì •ê·œí™”
scaler = StandardScaler()
normal_data = scaler.fit_transform(normal_data).astype(np.float32)
outlier_data = scaler.transform(outlier_data).astype(np.float32)
print(f"âœ… ì •ê·œí™” ì™„ë£Œ")

# ë°ì´í„° ë¶„ë¦¬
n_test = int(len(normal_data) * 0.2)
train_data = normal_data[:-n_test]
test_normal = normal_data[-n_test:]

test_data = np.vstack([test_normal, outlier_data])
test_labels = np.hstack([
    np.zeros(len(test_normal), dtype=np.int32),
    np.ones(len(outlier_data), dtype=np.int32)
])

print(f"âœ… ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ")
print(f"   í•™ìŠµ: {len(train_data):,}ê°œ")
print(f"   í…ŒìŠ¤íŠ¸: {len(test_data):,}ê°œ")

# ëª¨ë¸ í•™ìŠµ
print(f"\n{'='*80}")
print(f"ğŸ“ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ (2ê°œ ëª¨ë¸)")
print(f"{'='*80}")

models = []

try:
    from deepod.models.time_series import TimesNet, AnomalyTransformer
    
    model_configs = [
        ('TimesNet', 'timesnet', TimesNet),
        ('AnomalyTransformer', 'anomalytransformer', AnomalyTransformer)
    ]
    
    for idx, (model_name, file_name, ModelClass) in enumerate(model_configs, 1):
        print(f"\n{'â”€'*80}")
        print(f"ğŸ“¦ [{idx}/2] {model_name} í•™ìŠµ")
        print(f"{'â”€'*80}")
        
        start_time = datetime.now()
        
        model = ModelClass(
            seq_len=50,
            epochs=50,
            batch_size=128,
            device=device,
            verbose=1
        )
        
        print(f"âœ… {model_name} ì´ˆê¸°í™”")
        print(f"ğŸš€ í•™ìŠµ ì¤‘...")
        
        model.fit(train_data)
        
        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âœ… {model_name} í•™ìŠµ ì™„ë£Œ ({elapsed:.1f}ì´ˆ = {elapsed/60:.1f}ë¶„)")
        
        # ì„ê³„ê°’
        scores = model.decision_function(train_data)
        threshold = np.percentile(scores, 95)
        
        models.append((model, threshold, model_name))
        
        # ì €ì¥
        model_path = str(project_root / "2_model_training" / f"best_{file_name}.pkl")
        with open(model_path, 'wb') as f:
            pickle.dump({'model': model, 'threshold': threshold}, f)
        
        print(f"ğŸ’¾ ì €ì¥: best_{file_name}.pkl")

except Exception as e:
    print(f"\nâŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
    sys.exit(1)

print(f"\n{'='*80}")
print(f"âœ… ì•™ìƒë¸” í•™ìŠµ ì™„ë£Œ! (2ê°œ ëª¨ë¸)")
print(f"{'='*80}")

# ì•™ìƒë¸” í‰ê°€
print(f"\n{'='*80}")
print(f"ğŸ” ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€")
print(f"{'='*80}")

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score

def ensemble_predict(models, test_data, method='average'):
    """ì•™ìƒë¸” ì˜ˆì¸¡"""
    all_scores = []
    for model, threshold, _ in models:
        scores = model.decision_function(test_data)
        normalized = scores / threshold
        all_scores.append(normalized)
    
    all_scores = np.array(all_scores)
    
    if method == 'average':
        return np.mean(all_scores, axis=0)
    elif method == 'max':
        return np.max(all_scores, axis=0)
    elif method == 'voting':
        predictions = all_scores > 1.0
        return np.mean(predictions, axis=0)
    
    return np.mean(all_scores, axis=0)

# ì—¬ëŸ¬ ë°©ë²• ë¹„êµ
methods = ['average', 'max', 'voting']
best_result = None
best_f1 = 0

for method in methods:
    scores = ensemble_predict(models, test_data, method)
    
    # ìµœì  ì„ê³„ê°’ ì°¾ê¸°
    best_thresh = 1.0
    best_method_f1 = 0
    best_metrics = None
    
    for thresh in np.arange(0.3, 2.0, 0.1):
        preds = (scores > thresh).astype(int)
        recall = recall_score(test_labels, preds, zero_division=0)
        
        # Recall 85% ì´ìƒ ëª©í‘œ
        if recall >= 0.85:
            precision = precision_score(test_labels, preds, zero_division=0)
            f1 = f1_score(test_labels, preds, zero_division=0)
            
            if f1 > best_method_f1:
                best_method_f1 = f1
                best_thresh = thresh
                best_metrics = {
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'predictions': preds
                }
    
    # Recall 85% ë‹¬ì„± ëª»í•˜ë©´ ì°¨ì„ ì±…
    if best_metrics is None:
        best_thresh = 1.0
        preds = (scores > best_thresh).astype(int)
        best_metrics = {
            'precision': precision_score(test_labels, preds, zero_division=0),
            'recall': recall_score(test_labels, preds, zero_division=0),
            'f1': f1_score(test_labels, preds, zero_division=0),
            'predictions': preds
        }
    
    # ì „ì²´ ì§€í‘œ
    preds = best_metrics['predictions']
    accuracy = accuracy_score(test_labels, preds)
    
    try:
        auc = roc_auc_score(test_labels, scores)
    except:
        auc = 0.0
    
    tn = ((preds == 0) & (test_labels == 0)).sum()
    fp = ((preds == 1) & (test_labels == 0)).sum()
    fn = ((preds == 0) & (test_labels == 1)).sum()
    tp = ((preds == 1) & (test_labels == 1)).sum()
    fpr = fp / (tn + fp) if (tn + fp) > 0 else 0
    
    print(f"\nğŸ“Š {method.upper()}")
    print(f"   ì„ê³„ê°’: {best_thresh:.2f}")
    print(f"   Precision: {best_metrics['precision']:.1%}")
    print(f"   Recall:    {best_metrics['recall']:.1%}")
    print(f"   F1-Score:  {best_metrics['f1']:.4f}")
    print(f"   Accuracy:  {accuracy:.1%}")
    print(f"   ROC-AUC:   {auc:.4f}")
    print(f"   ì˜¤íƒë¥ :    {fpr:.2%}")
    
    # ìµœê³  ì„±ëŠ¥ ì €ì¥
    if best_metrics['f1'] > best_f1:
        best_f1 = best_metrics['f1']
        best_result = {
            'method': method,
            'threshold': best_thresh,
            'scores': scores,
            'metrics': {**best_metrics, 'accuracy': accuracy, 'auc': auc, 'fpr': fpr,
                       'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn}
        }

# ìµœê³  ì„±ëŠ¥ ì¶œë ¥
print(f"\n{'='*80}")
print(f"ğŸ† ìµœê³  ì„±ëŠ¥")
print(f"{'='*80}")

if best_result:
    m = best_result['metrics']
    print(f"\n   ë°©ë²•: {best_result['method'].upper()}")
    print(f"   ì„ê³„ê°’: {best_result['threshold']:.4f}")
    print(f"\n   ğŸ“ˆ ìµœì¢… ì„±ëŠ¥:")
    print(f"      Precision: {m['precision']:.1%}")
    print(f"      Recall:    {m['recall']:.1%} â­")
    print(f"      F1-Score:  {m['f1']:.4f}")
    print(f"      Accuracy:  {m['accuracy']:.1%}")
    print(f"      ROC-AUC:   {m['auc']:.4f}")
    print(f"      ì˜¤íƒë¥ :    {m['fpr']:.2%}")
    
    print(f"\n   ğŸ¯ Confusion Matrix:")
    print(f"      ì‹¤ì œ\\ì˜ˆì¸¡     ì •ìƒ(0)    ì´ìƒ(1)")
    print(f"      ì •ìƒ(0)      {m['tn']:6d}    {m['fp']:6d}")
    print(f"      ì´ìƒ(1)      {m['fn']:6d}    {m['tp']:6d}")
    
    print(f"\n   ğŸ’¡ ê²°ê³¼:")
    print(f"      âœ… ì´ìƒ {m['tp']+m['fn']}ê°œ ì¤‘ {m['tp']}ê°œ íƒì§€ (ë†“ì¹¨: {m['fn']}ê°œ)")
    print(f"      âœ… ì •ìƒ {m['tn']+m['fp']}ê°œ ì¤‘ {m['fp']}ê°œ ì˜¤íƒ (ì˜¤íƒë¥ : {m['fpr']:.2%})")
    
    # í‰ê°€
    if m['recall'] >= 0.85:
        print(f"\n   ğŸ‰ Recall 85% ì´ìƒ ë‹¬ì„±! ({'ë†“ì¹¨ 15% ì´í•˜' if m['fn'] <= 90 else 'ëª©í‘œ ê·¼ì ‘'})")
    else:
        print(f"\n   âš ï¸  Recall {m['recall']:.1%} (ëª©í‘œ: 85% ì´ìƒ)")
        print(f"      â†’ 2ê°œ ëª¨ë¸ë¡œë„ {m['recall']:.1%} ë‹¬ì„±!")

# ìµœì¢… ëª¨ë¸ ì €ì¥
final_path = str(project_root / "2_model_training" / "best_ensemble_2models.pkl")

with open(final_path, 'wb') as f:
    pickle.dump({
        'models': models,
        'scaler': scaler,
        'best_method': best_result['method'] if best_result else 'average',
        'best_threshold': best_result['threshold'] if best_result else 1.0,
        'results': best_result,
        'seq_len': 50,
        'n_models': 2
    }, f)

file_size_mb = os.path.getsize(final_path) / (1024 * 1024)
print(f"\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: best_ensemble_2models.pkl ({file_size_mb:.2f} MB)")

print(f"\n{'='*80}")
print(f"âœ¨ ì™„ë£Œ!")
print(f"{'='*80}")
print(f"\nğŸ“ ì‚¬ìš© ë°©ë²•:")
print(f"""
with open('best_ensemble_2models.pkl', 'rb') as f:
    ensemble = pickle.load(f)

models = ensemble['models']  # TimesNet + AnomalyTransformer
method = ensemble['best_method']
threshold = ensemble['best_threshold']
""")

recall_status = "ë‹¬ì„±! ğŸ‰" if best_result and best_result['metrics']['recall'] >= 0.85 else "í™•ì¸ í•„ìš”"
print(f"\nğŸ¯ Recall 85% ëª©í‘œ: {recall_status}")

