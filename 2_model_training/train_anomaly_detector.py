"""
DeepOD TimesNet ì‹œê³„ì—´ ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python train_anomaly_detector.py --data_path data/sensor_data.csv --epochs 20

DeepOD TimesNetì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ (n_samples, seq_len, n_features) í˜•íƒœë¡œ ë°›ìŠµë‹ˆë‹¤.
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import torch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / "4_agent_system"))

from models.anomaly_detector import AnomalyDetectionModel


def get_device():
    """
    GPU ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ì„¤ì • (A100 ìµœì í™”)
    """
    if torch.cuda.is_available():
        device = 'cuda'
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"âœ… GPU ê°ì§€: {gpu_name}")
        print(f"   GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")
        
        # A100 ê°ì§€
        if 'A100' in gpu_name:
            print(f"   ğŸš€ A100 GPU ê°ì§€ - ìµœì í™” ëª¨ë“œ í™œì„±í™”")
            # CUDA ìµœì í™” ì„¤ì •
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
    else:
        device = 'cpu'
        print(f"âš ï¸  GPU ì—†ìŒ - CPU ëª¨ë“œ")
    
    return device


def get_optimal_batch_size(device: str, seq_len: int, n_features: int, default_batch_size: int = 32):
    """
    GPU ë©”ëª¨ë¦¬ì— ë§ëŠ” ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
    """
    if device == 'cpu':
        return min(default_batch_size, 16)
    
    try:
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        
        # A100 40GB ê¸°ì¤€ ìµœì  ë°°ì¹˜ í¬ê¸°
        if gpu_memory_gb >= 40:  # A100 40GB
            # ë©”ëª¨ë¦¬ ì—¬ìœ ê°€ ìˆìœ¼ë¯€ë¡œ í° ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
            if seq_len <= 50:
                optimal_batch = 128
            elif seq_len <= 100:
                optimal_batch = 64
            else:
                optimal_batch = 32
        elif gpu_memory_gb >= 24:  # A100 24GB or similar
            optimal_batch = 64
        elif gpu_memory_gb >= 16:  # V100 or similar
            optimal_batch = 32
        else:
            optimal_batch = 16
        
        print(f"   ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch} (GPU ë©”ëª¨ë¦¬: {gpu_memory_gb:.1f}GB ê¸°ì¤€)")
        return optimal_batch
    except:
        return default_batch_size


def create_synthetic_time_series_data(
    n_samples: int = 5000,
    seq_len: int = 50,
    noise_level: float = 0.1
) -> np.ndarray:
    """
    ì‹œë®¬ë ˆì´ì…˜ ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
    
    Args:
        n_samples: ì „ì²´ ìƒ˜í”Œ ìˆ˜
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
        noise_level: ë…¸ì´ì¦ˆ ë ˆë²¨
    
    Returns:
        (n_samples, seq_len, n_features) í˜•íƒœì˜ numpy ë°°ì—´
    """
    print(f"ğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ì‹œê³„ì—´ ë°ì´í„° ìƒì„± ì¤‘...")
    print(f"   - ìƒ˜í”Œ ìˆ˜: {n_samples}")
    print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")
    print(f"   - ë…¸ì´ì¦ˆ ë ˆë²¨: {noise_level}")
    
    n_features = 4  # temperature, pressure, vibration, cycle_time
    
    # ì „ì²´ ì‹œê³„ì—´ ë°ì´í„° ìƒì„± (ì •ìƒ íŒ¨í„´)
    total_length = n_samples + seq_len - 1
    
    # ì •ìƒ íŒ¨í„´: ì£¼ê¸°ì  ë³€ë™ + íŠ¸ë Œë“œ
    time_points = np.arange(total_length)
    
    # ê° ì„¼ì„œë³„ ì •ìƒ íŒ¨í„´
    data = np.zeros((total_length, n_features))
    
    # 1. ì˜¨ë„: ì£¼ê¸°ì  ë³€ë™ (200Â°C ê¸°ì¤€)
    data[:, 0] = 200 + 5 * np.sin(2 * np.pi * time_points / 100) + \
                 2 * np.sin(2 * np.pi * time_points / 50) + \
                 np.random.normal(0, 2, total_length)
    
    # 2. ì••ë ¥: ì£¼ê¸°ì  ë³€ë™ (120 bar ê¸°ì¤€)
    data[:, 1] = 120 + 3 * np.sin(2 * np.pi * time_points / 80) + \
                 np.random.normal(0, 1.5, total_length)
    
    # 3. ì§„ë™: ì•ˆì •ì  íŒ¨í„´ (1.0 mm/s ê¸°ì¤€)
    data[:, 2] = 1.0 + 0.1 * np.sin(2 * np.pi * time_points / 120) + \
                 np.random.normal(0, 0.15, total_length)
    
    # 4. ì‚¬ì´í´íƒ€ì„: ì•ˆì •ì  íŒ¨í„´ (50ì´ˆ ê¸°ì¤€)
    data[:, 3] = 50 + 1 * np.sin(2 * np.pi * time_points / 90) + \
                 np.random.normal(0, 1, total_length)
    
    # ë…¸ì´ì¦ˆ ì¶”ê°€
    data += np.random.normal(0, noise_level, data.shape)
    
    # Sliding windowë¡œ ì‹œí€€ìŠ¤ ìƒì„±
    sequences = []
    for i in range(n_samples):
        seq = data[i:i+seq_len]  # (seq_len, n_features)
        sequences.append(seq)
    
    X = np.array(sequences, dtype=np.float32)  # (n_samples, seq_len, n_features)
    
    print(f"âœ… ë°ì´í„° ìƒì„± ì™„ë£Œ: {X.shape}, dtype: {X.dtype}")
    return X


def load_moldset_data(
    csv_path: str,
    seq_len: int = 50,
    use_label: bool = True,
    use_normal_only: bool = False,
    chunk_size: int = None
) -> tuple:
    """
    Moldset ë°ì´í„°ì…‹ ë¡œë“œ ë° ì‹œê³„ì—´ ë³€í™˜ (ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”)
    
    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
        use_label: Label ì •ë³´ ì‚¬ìš© ì—¬ë¶€ (Trueë©´ PassOrFail ì»¬ëŸ¼ ì²˜ë¦¬)
        use_normal_only: ì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš© (Labelì´ ìˆì„ ë•Œë§Œ ìœ íš¨)
        chunk_size: ì²­í¬ í¬ê¸° (ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ìš©, Noneì´ë©´ ì „ì²´ ë¡œë“œ)
    
    Returns:
        (sequences, labels) í˜•íƒœì˜ tuple
        - sequences: (n_samples, seq_len, n_features) í˜•íƒœì˜ numpy ë°°ì—´
        - labels: (n_samples,) í˜•íƒœì˜ numpy ë°°ì—´ (use_label=Falseë©´ None)
    """
    print(f"ğŸ“‚ Moldset ë°ì´í„°ì…‹ ë¡œë“œ: {csv_path}")
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)
    print(f"   íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
    
    # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ (100MB ì´ìƒ)
    if file_size_mb > 100 and chunk_size is None:
        chunk_size = 50000  # ê¸°ë³¸ ì²­í¬ í¬ê¸°
        print(f"   âš ï¸  ëŒ€ìš©ëŸ‰ íŒŒì¼ ê°ì§€ - ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ í™œì„±í™”")
    
    # CSV íŒŒì¼ ì½ê¸° (ì²­í¬ ì²˜ë¦¬ ë˜ëŠ” ì „ì²´ ë¡œë“œ)
    if chunk_size:
        print(f"   ğŸ“– ì²­í¬ ë‹¨ìœ„ ë¡œë“œ ì¤‘... (ì²­í¬ í¬ê¸°: {chunk_size:,}í–‰)")
        chunks = []
        for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size)):
            chunks.append(chunk)
            if (i + 1) % 10 == 0:
                print(f"      ì²­í¬ {i+1} ë¡œë“œ ì™„ë£Œ...")
        df = pd.concat(chunks, ignore_index=True)
        print(f"   âœ… ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    else:
        df = pd.read_csv(csv_path)
    
    print(f"   ì›ë³¸ ë°ì´í„° í˜•íƒœ: {df.shape}")
    
    # Label ì»¬ëŸ¼ í™•ì¸ ë° ë¶„ë¦¬
    label_col = None
    if use_label:
        label_candidates = ['PassOrFail', 'label', 'Label', 'anomaly', 'Anomaly', 'target', 'Target']
        for col in label_candidates:
            if col in df.columns:
                label_col = col
                break
        
        if label_col:
            labels = df[label_col].values
            df = df.drop(columns=[label_col])
            print(f"   Label ì»¬ëŸ¼ ë°œê²¬: {label_col}")
            print(f"   - ì •ìƒ(0): {(labels == 0).sum()}ê°œ")
            print(f"   - ì´ìƒ(1): {(labels == 1).sum()}ê°œ")
        else:
            print(f"   âš ï¸  Label ì»¬ëŸ¼ ì—†ìŒ (Unsupervised í•™ìŠµ)")
            labels = None
    else:
        labels = None
    
    # ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì œê±° (ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ì¸ë±ìŠ¤ì¸ ê²½ìš°)
    if df.columns[0] == 'Unnamed: 0' or df.columns[0] == '':
        df = df.drop(columns=[df.columns[0]])
    
    # ì„¼ì„œ ì»¬ëŸ¼ ìë™ ë§¤í•‘
    sensor_mapping = {
        'ì˜¨ë„': [],
        'ì••ë ¥': [],
        'ì‚¬ì´í´íƒ€ì„': [],
        'ê¸°íƒ€': []
    }
    
    # ì˜¨ë„ ì„¼ì„œ ì°¾ê¸°
    temp_keywords = ['temperature', 'Temperature', 'Temp', 'temp', 'Barrel_Temperature', 
                     'Hopper_Temperature', 'Mold_Temperature']
    for col in df.columns:
        if any(kw in col for kw in temp_keywords):
            sensor_mapping['ì˜¨ë„'].append(col)
    
    # ì••ë ¥ ì„¼ì„œ ì°¾ê¸°
    pressure_keywords = ['pressure', 'Pressure', 'Press', 'press', 'Injection_Pressure', 
                         'Back_Pressure', 'Switch_Over_Pressure']
    for col in df.columns:
        if any(kw in col for kw in pressure_keywords):
            sensor_mapping['ì••ë ¥'].append(col)
    
    # ì‚¬ì´í´íƒ€ì„ ì°¾ê¸°
    cycle_keywords = ['Cycle_Time', 'cycle_time', 'CycleTime', 'cycle', 'Cycle']
    for col in df.columns:
        if any(kw in col for kw in cycle_keywords):
            sensor_mapping['ì‚¬ì´í´íƒ€ì„'].append(col)
            break  # í•˜ë‚˜ë§Œ ì‚¬ìš©
    
    # ê¸°íƒ€ ì„¼ì„œ (ì†ë„, RPM, ì‹œê°„ ë“±)
    other_keywords = ['Speed', 'RPM', 'Time', 'Position', 'Position']
    for col in df.columns:
        if col not in sensor_mapping['ì˜¨ë„'] and col not in sensor_mapping['ì••ë ¥'] and \
           col not in sensor_mapping['ì‚¬ì´í´íƒ€ì„']:
            if any(kw in col for kw in other_keywords):
                sensor_mapping['ê¸°íƒ€'].append(col)
    
    # ì‚¬ìš©í•  ì„¼ì„œ ì»¬ëŸ¼ ì„ íƒ
    selected_cols = []
    
    # ì˜¨ë„: í‰ê· ê°’ ì‚¬ìš© (ì—¬ëŸ¬ ì˜¨ë„ ì„¼ì„œê°€ ìˆìœ¼ë©´ í‰ê· )
    if sensor_mapping['ì˜¨ë„']:
        if len(sensor_mapping['ì˜¨ë„']) == 1:
            selected_cols.append(sensor_mapping['ì˜¨ë„'][0])
            print(f"   âœ… ì˜¨ë„ ì„¼ì„œ: {sensor_mapping['ì˜¨ë„'][0]}")
        else:
            # ì—¬ëŸ¬ ì˜¨ë„ ì„¼ì„œì˜ í‰ê·  ê³„ì‚°
            df['temperature_avg'] = df[sensor_mapping['ì˜¨ë„']].mean(axis=1)
            selected_cols.append('temperature_avg')
            print(f"   âœ… ì˜¨ë„ ì„¼ì„œ: {len(sensor_mapping['ì˜¨ë„'])}ê°œ â†’ í‰ê· ê°’ ì‚¬ìš©")
    else:
        print(f"   âš ï¸  ì˜¨ë„ ì„¼ì„œ ì—†ìŒ")
    
    # ì••ë ¥: ìµœëŒ€ê°’ ë˜ëŠ” í‰ê· ê°’ ì‚¬ìš©
    if sensor_mapping['ì••ë ¥']:
        if len(sensor_mapping['ì••ë ¥']) == 1:
            selected_cols.append(sensor_mapping['ì••ë ¥'][0])
            print(f"   âœ… ì••ë ¥ ì„¼ì„œ: {sensor_mapping['ì••ë ¥'][0]}")
        else:
            # ì—¬ëŸ¬ ì••ë ¥ ì„¼ì„œì˜ í‰ê·  ê³„ì‚°
            df['pressure_avg'] = df[sensor_mapping['ì••ë ¥']].mean(axis=1)
            selected_cols.append('pressure_avg')
            print(f"   âœ… ì••ë ¥ ì„¼ì„œ: {len(sensor_mapping['ì••ë ¥'])}ê°œ â†’ í‰ê· ê°’ ì‚¬ìš©")
    else:
        print(f"   âš ï¸  ì••ë ¥ ì„¼ì„œ ì—†ìŒ")
    
    # ì‚¬ì´í´íƒ€ì„
    if sensor_mapping['ì‚¬ì´í´íƒ€ì„']:
        selected_cols.append(sensor_mapping['ì‚¬ì´í´íƒ€ì„'][0])
        print(f"   âœ… ì‚¬ì´í´íƒ€ì„: {sensor_mapping['ì‚¬ì´í´íƒ€ì„'][0]}")
    else:
        print(f"   âš ï¸  ì‚¬ì´í´íƒ€ì„ ì—†ìŒ")
    
    # ê¸°íƒ€ ì„¼ì„œ (ì†ë„ ë“±) - ìˆ«ìí˜•ë§Œ ì¶”ê°€ (ë¬¸ìì—´/ë‚ ì§œ ì œì™¸)
    if sensor_mapping['ê¸°íƒ€']:
        numeric_count = 0
        for col in sensor_mapping['ê¸°íƒ€']:
            if numeric_count >= 2:  # ìµœëŒ€ 2ê°œë§Œ
                break
            # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì¶”ê°€ (ë¬¸ìì—´/ë‚ ì§œ ì œì™¸)
            try:
                # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸
                test_val = pd.to_numeric(df[col], errors='coerce')
                if not test_val.isna().all():  # ëª¨ë‘ NaNì´ ì•„ë‹ˆë©´ ìˆ«ìí˜•
                    selected_cols.append(col)
                    print(f"   âœ… ì¶”ê°€ ì„¼ì„œ: {col}")
                    numeric_count += 1
                else:
                    print(f"   âš ï¸  {col}: ìˆ«ìí˜•ì´ ì•„ë‹˜ (ì œì™¸)")
            except:
                print(f"   âš ï¸  {col}: ìˆ«ìí˜•ì´ ì•„ë‹˜ (ì œì™¸)")
    
    if not selected_cols:
        raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ì„¼ì„œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"\n   ìµœì¢… ì‚¬ìš© ì»¬ëŸ¼ ({len(selected_cols)}ê°œ): {selected_cols}")
    
    # NaN ê°’ ì²˜ë¦¬ ë° ìˆ«ìí˜• ë³€í™˜
    data = df[selected_cols].copy()
    
    # ëª¨ë“  ì»¬ëŸ¼ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ (ë¬¸ìì—´/ë‚ ì§œ ì œì™¸)
    numeric_cols = []
    for col in data.columns:
        try:
            # ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
            data[col] = pd.to_numeric(data[col], errors='coerce')
            # ë³€í™˜ í›„ ëª¨ë‘ NaNì´ ì•„ë‹ˆë©´ ì‚¬ìš©
            if not data[col].isna().all():
                numeric_cols.append(col)
            else:
                print(f"   âš ï¸  {col}: ìˆ«ì ë³€í™˜ ì‹¤íŒ¨, ì œì™¸")
        except:
            print(f"   âš ï¸  {col}: ìˆ«ì ë³€í™˜ ì‹¤íŒ¨, ì œì™¸")
    
    # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì‚¬ìš©
    if len(numeric_cols) == 0:
        raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ«ìí˜• ì„¼ì„œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    data = data[numeric_cols]
    if len(numeric_cols) < len(selected_cols):
        print(f"   âš ï¸  ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì‚¬ìš©: {len(numeric_cols)}ê°œ (ì›ë˜ {len(selected_cols)}ê°œ)")
    
    nan_before = data.isna().sum().sum()
    if nan_before > 0:
        print(f"   âš ï¸  NaN ê°’ ë°œê²¬: {nan_before}ê°œ")
        # ì „ë°© ì±„ìš°ê¸° í›„ í›„ë°© ì±„ìš°ê¸° (pandas ìµœì‹  ë²„ì „ í˜¸í™˜)
        try:
            data = data.ffill().bfill()
        except AttributeError:
            # êµ¬ë²„ì „ pandas í˜¸í™˜
            data = data.fillna(method='ffill').fillna(method='bfill')
        # ê·¸ë˜ë„ NaNì´ ìˆìœ¼ë©´ 0ìœ¼ë¡œ ì±„ìš°ê¸°
        data = data.fillna(0)
        print(f"   âœ… NaN ê°’ ì²˜ë¦¬ ì™„ë£Œ")
    
    # ì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš© ì˜µì…˜
    if use_normal_only and labels is not None:
        normal_indices = labels == 0
        data = data[normal_indices]
        if labels is not None:
            labels = labels[normal_indices]
        print(f"   âœ… ì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš©: {len(data)}ê°œ ìƒ˜í”Œ")
    
    # ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (float32ë¡œ ëª…ì‹œì  ë³€í™˜)
    data_values = data.values.astype(np.float32)  # (n_timesteps, n_features)
    
    print(f"\n   ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {data_values.shape}")
    print(f"   - ì‹œê³„ì—´ ê¸¸ì´: {len(data_values)}")
    print(f"   - íŠ¹ì§• ìˆ˜: {data_values.shape[1]}")
    print(f"   - ë°ì´í„° íƒ€ì…: {data_values.dtype}")
    
    # Sliding windowë¡œ ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    print(f"\n   ğŸ”„ Sliding Windowë¡œ ì‹œê³„ì—´ ë³€í™˜ ì¤‘... (seq_len={seq_len})")
    
    n_samples = len(data_values) - seq_len + 1
    n_features = data_values.shape[1]
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì‹œí€€ìŠ¤ ìƒì„±
    print(f"   ì˜ˆìƒ ì‹œí€€ìŠ¤ ìˆ˜: {n_samples:,}ê°œ")
    
    # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ê²½ìš° ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    if n_samples > 100000:
        print(f"   âš ï¸  ëŒ€ìš©ëŸ‰ ë°ì´í„° ê°ì§€ - ì²­í¬ ë‹¨ìœ„ ë³€í™˜")
        chunk_size = 50000
        sequences_chunks = []
        labels_chunks = []
        
        for start_idx in range(0, n_samples, chunk_size):
            end_idx = min(start_idx + chunk_size, n_samples)
            chunk_sequences = []
            chunk_labels = []
            
            for i in range(start_idx, end_idx):
                seq = data_values[i:i+seq_len]  # (seq_len, n_features)
                chunk_sequences.append(seq)
                
                if labels is not None:
                    chunk_labels.append(labels[i + seq_len - 1])
            
            sequences_chunks.append(np.array(chunk_sequences, dtype=np.float32))
            if labels is not None:
                labels_chunks.append(np.array(chunk_labels, dtype=np.int64))
            
            print(f"      ì²­í¬ ë³€í™˜ ì™„ë£Œ: {end_idx:,}/{n_samples:,}")
        
        X = np.concatenate(sequences_chunks, axis=0)
        y = np.concatenate(labels_chunks, axis=0) if labels is not None else None
    else:
        # ì†Œê·œëª¨ ë°ì´í„°ëŠ” ì¼ë°˜ ì²˜ë¦¬
        sequences = []
        sequence_labels = []
        
        for i in range(n_samples):
            seq = data_values[i:i+seq_len]  # (seq_len, n_features)
            sequences.append(seq)
            
            if labels is not None:
                sequence_labels.append(labels[i + seq_len - 1])
        
        X = np.array(sequences, dtype=np.float32)  # (n_samples, seq_len, n_features)
        y = np.array(sequence_labels, dtype=np.int64) if labels is not None else None
    
    print(f"   âœ… ì‹œê³„ì—´ ë³€í™˜ ì™„ë£Œ: {X.shape}")
    if y is not None:
        print(f"   - ì •ìƒ ì‹œí€€ìŠ¤: {(y == 0).sum()}ê°œ")
        print(f"   - ì´ìƒ ì‹œí€€ìŠ¤: {(y == 1).sum()}ê°œ")
    
    return X, y


def load_time_series_from_csv(
    csv_path: str,
    seq_len: int = 50,
    columns: list = None,
    use_moldset_format: bool = True
) -> np.ndarray:
    """
    CSV íŒŒì¼ì—ì„œ ì‹œê³„ì—´ ë°ì´í„° ë¡œë“œ (ê¸°ì¡´ í•¨ìˆ˜ì™€ í˜¸í™˜ì„± ìœ ì§€)
    
    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
        columns: ì‚¬ìš©í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìë™ ì„ íƒ)
        use_moldset_format: Moldset ë°ì´í„°ì…‹ í˜•ì‹ ìë™ ê°ì§€ ë° ì‚¬ìš©
    
    Returns:
        (n_samples, seq_len, n_features) í˜•íƒœì˜ numpy ë°°ì—´
    """
    # Moldset í˜•ì‹ ìë™ ê°ì§€
    if use_moldset_format:
        # Moldset ë°ì´í„°ì…‹ íŠ¹ì§• í™•ì¸
        df_sample = pd.read_csv(csv_path, nrows=5)
        moldset_indicators = [
            'PassOrFail' in df_sample.columns,
            'Cycle_Time' in df_sample.columns,
            'Barrel_Temperature' in str(df_sample.columns),
            'Injection_Pressure' in str(df_sample.columns)
        ]
        
        if any(moldset_indicators):
            print(f"   ğŸ” Moldset ë°ì´í„°ì…‹ í˜•ì‹ ê°ì§€")
            sequences, _ = load_moldset_data(
                csv_path=csv_path,
                seq_len=seq_len,
                use_label=False,  # Unsupervised í•™ìŠµìš©
                use_normal_only=False
            )
            return sequences
    
    # ê¸°ì¡´ ë¡œì§ (ì¼ë°˜ CSV íŒŒì¼)
    print(f"ğŸ“‚ CSV íŒŒì¼ ë¡œë“œ: {csv_path}")
    
    df = pd.read_csv(csv_path)
    
    # ì»¬ëŸ¼ ì„ íƒ
    if columns is None:
        # ìë™ìœ¼ë¡œ ì„¼ì„œ ì»¬ëŸ¼ ì°¾ê¸°
        sensor_cols = ['temperature', 'pressure', 'vibration', 'cycle_time']
        columns = [col for col in sensor_cols if col in df.columns]
    
    if not columns:
        raise ValueError("ì„¼ì„œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"   ì‚¬ìš© ì»¬ëŸ¼: {columns}")
    
    # ë°ì´í„° ì •ê·œí™” (ì„ íƒì‚¬í•­) ë° float32 ë³€í™˜
    data = df[columns].values.astype(np.float32)
    
    # NaN ì²˜ë¦¬
    if np.isnan(data).any():
        data_df = pd.DataFrame(data)
        try:
            data = data_df.ffill().bfill().fillna(0).values.astype(np.float32)
        except AttributeError:
            # êµ¬ë²„ì „ pandas í˜¸í™˜
            data = data_df.fillna(method='ffill').fillna(method='bfill').fillna(0).values.astype(np.float32)
    
    # Sliding windowë¡œ ì‹œí€€ìŠ¤ ìƒì„±
    sequences = []
    for i in range(len(data) - seq_len + 1):
        seq = data[i:i+seq_len]  # (seq_len, n_features)
        sequences.append(seq)
    
    X = np.array(sequences, dtype=np.float32)  # (n_samples, seq_len, n_features)
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {X.shape}")
    return X


def train_model(
    train_data: np.ndarray,
    model_type: str = "TimesNet",
    seq_len: int = 50,
    epochs: int = 20,
    batch_size: int = None,
    model_save_path: str = None,
    device: str = None
):
    """
    DeepOD ëª¨ë¸ í•™ìŠµ (A100 GPU ìµœì í™”)
    
    Args:
        train_data: (n_samples, seq_len, n_features) í˜•íƒœì˜ í•™ìŠµ ë°ì´í„°
        model_type: ëª¨ë¸ íƒ€ì… ('TimesNet', 'AnomalyTransformer', 'TranAD')
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
        epochs: í•™ìŠµ ì—í¬í¬
        batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ ìë™ ìµœì í™”)
        model_save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        device: ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu', Noneì´ë©´ ìë™ ê°ì§€)
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“ DeepOD {model_type} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print(f"{'='*80}")
    print(f"   - ë°ì´í„° í˜•íƒœ: {train_data.shape}")
    print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")
    print(f"   - ì—í¬í¬: {epochs}")
    
    # GPU ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
    if device is None:
        device = get_device()
    else:
        print(f"   ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°°ì¹˜ í¬ê¸° ìë™ ìµœì í™”
    if batch_size is None:
        n_features = train_data.shape[2]
        batch_size = get_optimal_batch_size(device, seq_len, n_features)
    else:
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size} (ì‚¬ìš©ì ì§€ì •)")
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    if model_save_path is None:
        model_save_path = str(project_root / "2_model_training" / f"anomaly_model_{model_type.lower()}.pkl")
    
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    try:
        from deepod.models.time_series import TimesNet, AnomalyTransformer, TranAD
        
        if model_type == "TimesNet":
            model = TimesNet(
                seq_len=seq_len,
                epochs=epochs,
                batch_size=batch_size,
                device=device,
                verbose=1
            )
        elif model_type == "AnomalyTransformer":
            model = AnomalyTransformer(
                seq_len=seq_len,
                epochs=epochs,
                batch_size=batch_size,
                device=device,
                verbose=1
            )
        elif model_type == "TranAD":
            model = TranAD(
                seq_len=seq_len,
                epochs=epochs,
                batch_size=batch_size,
                device=device,
                verbose=1
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
        
        print(f"âœ… {model_type} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
    except ImportError:
        print("âŒ DeepOD ë¯¸ì„¤ì¹˜: pip install deepod")
        return
    
    # í•™ìŠµ ì‹¤í–‰
    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘...")
    start_time = datetime.now()
    
    # ë°ì´í„° í˜•íƒœ í™•ì¸ ë° ë³€í™˜
    print(f"   ì…ë ¥ ë°ì´í„° í˜•íƒœ: {train_data.shape}")
    print(f"   ì…ë ¥ ë°ì´í„° dtype: {train_data.dtype}")
    
    # DeepOD TimesNetì€ ë‚´ë¶€ì ìœ¼ë¡œ sliding windowë¥¼ ì ìš©í•˜ë¯€ë¡œ
    # ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„° (n_timesteps, n_features) í˜•íƒœë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤
    # í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ì´ë¯¸ windowingëœ (n_samples, seq_len, n_features) í˜•íƒœë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤
    # ë”°ë¼ì„œ ì›ë³¸ ì‹œê³„ì—´ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤
    
    if len(train_data.shape) == 3:
        # (n_samples, seq_len, n_features) -> ì›ë³¸ ì‹œê³„ì—´ë¡œ ë³€í™˜
        # DeepOD TimesNetì€ ë‚´ë¶€ì ìœ¼ë¡œ sliding windowë¥¼ ì ìš©í•˜ë¯€ë¡œ ì›ë³¸ ì‹œê³„ì—´ì´ í•„ìš”
        print(f"   âš ï¸  Windowed ë°ì´í„° ê°ì§€: {train_data.shape}")
        print(f"   ì›ë³¸ ì‹œê³„ì—´ë¡œ ë³€í™˜ ì¤‘...")
        
        n_samples, seq_len_actual, n_features = train_data.shape
        
        # ì›ë³¸ ì‹œê³„ì—´ ë³µì›: ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ì‹œì  + ë‚˜ë¨¸ì§€ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹œì ë§Œ
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°©ë²•
        original_length = seq_len_actual + (n_samples - 1)  # ì²« ì‹œí€€ìŠ¤ ê¸¸ì´ + ë‚˜ë¨¸ì§€ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹œì ë“¤
        train_data_original = np.zeros((original_length, n_features), dtype=np.float32)
        
        # ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤: ëª¨ë“  ì‹œì  ë³µì‚¬
        train_data_original[:seq_len_actual] = train_data[0]
        
        # ë‚˜ë¨¸ì§€ ì‹œí€€ìŠ¤: ë§ˆì§€ë§‰ ì‹œì ë§Œ ë³µì‚¬ (ì¤‘ë³µ ì œê±°)
        for i in range(1, n_samples):
            train_data_original[seq_len_actual + i - 1] = train_data[i, -1]
        
        print(f"   âœ… ì›ë³¸ ì‹œê³„ì—´ ë³€í™˜ ì™„ë£Œ: {train_data_original.shape}")
        print(f"   - ì‹œê³„ì—´ ê¸¸ì´: {train_data_original.shape[0]}")
        print(f"   - íŠ¹ì§• ìˆ˜: {train_data_original.shape[1]}")
        
        train_data = train_data_original
    elif len(train_data.shape) == 2:
        # ì´ë¯¸ ì›ë³¸ ì‹œê³„ì—´ í˜•íƒœ (n_timesteps, n_features)
        print(f"   âœ… ì›ë³¸ ì‹œê³„ì—´ í˜•íƒœ í™•ì¸: {train_data.shape}")
        if train_data.dtype != np.float32:
            train_data = train_data.astype(np.float32)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•íƒœ: {train_data.shape}")
    
    print(f"   ìµœì¢… ë°ì´í„° í˜•íƒœ: {train_data.shape}")
    print(f"   ì˜ˆìƒ í˜•íƒœ: (n_timesteps, n_features)")
    
    model.fit(train_data)
    
    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"âœ… í•™ìŠµ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ)")
    
    # ì„ê³„ê°’ ì„¤ì • (ì •ìƒ ë°ì´í„°ì˜ 95% percentile)
    scores = model.decision_function(train_data)
    threshold = np.percentile(scores, 95)
    
    print(f"\nğŸ“Š í•™ìŠµ ê²°ê³¼:")
    print(f"   - ì„ê³„ê°’: {threshold:.4f}")
    print(f"   - ìŠ¤ì½”ì–´ ë²”ìœ„: [{scores.min():.4f}, {scores.max():.4f}]")
    print(f"   - ìŠ¤ì½”ì–´ í‰ê· : {scores.mean():.4f}")
    
    # ëª¨ë¸ ì €ì¥
    import pickle
    with open(model_save_path, 'wb') as f:
        pickle.dump({
            'model': model,
            'threshold': threshold,
            'model_type': model_type,
            'seq_len': seq_len,
            'train_shape': train_data.shape
        }, f)
    
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_save_path}")
    
    return model, threshold


def test_model(model, threshold, test_data: np.ndarray):
    """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print(f"\n{'='*80}")
    print(f"ğŸ” ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print(f"{'='*80}")
    
    scores = model.decision_function(test_data)
    predictions = scores > threshold
    
    print(f"   - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_data)}")
    print(f"   - ì´ìƒ íƒì§€: {predictions.sum()}ê°œ ({predictions.sum()/len(predictions)*100:.1f}%)")
    print(f"   - ì •ìƒ: {(~predictions).sum()}ê°œ ({(~predictions).sum()/len(predictions)*100:.1f}%)")
    
    return scores, predictions


def main():
    parser = argparse.ArgumentParser(description="DeepOD TimesNet í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--data_path",
        type=str,
        default=None,
        help="CSV íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì‚¬ìš©)"
    )
    parser.add_argument(
        "--model_type",
        type=str,
        default="TimesNet",
        choices=["TimesNet", "AnomalyTransformer", "TranAD"],
        help="ëª¨ë¸ íƒ€ì…"
    )
    parser.add_argument(
        "--seq_len",
        type=int,
        default=50,
        help="ì‹œí€€ìŠ¤ ê¸¸ì´"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=20,
        help="í•™ìŠµ ì—í¬í¬"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=None,
        help="ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ìë™ ìµœì í™”, ê¸°ë³¸ê°’: A100 40GB ê¸°ì¤€ 128)"
    )
    parser.add_argument(
        "--n_samples",
        type=int,
        default=5000,
        help="ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (CSV ì‚¬ìš© ì‹œ ë¬´ì‹œ)"
    )
    parser.add_argument(
        "--model_save_path",
        type=str,
        default=None,
        help="ëª¨ë¸ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--use_label",
        action="store_true",
        help="Label ì •ë³´ ì‚¬ìš© (PassOrFail ë“±)"
    )
    parser.add_argument(
        "--use_normal_only",
        action="store_true",
        help="ì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš© (Labelì´ ìˆì„ ë•Œë§Œ ìœ íš¨)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        choices=["cuda", "cpu"],
        help="ë””ë°”ì´ìŠ¤ ì§€ì • (Noneì´ë©´ ìë™ ê°ì§€)"
    )
    
    args = parser.parse_args()
    
    # GPU ë””ë°”ì´ìŠ¤ í™•ì¸
    device = get_device() if args.device is None else args.device
    
    # ë°ì´í„° ë¡œë“œ
    if args.data_path and os.path.exists(args.data_path):
        # Moldset ë°ì´í„°ì…‹ í˜•ì‹ ìë™ ê°ì§€
        df_sample = pd.read_csv(args.data_path, nrows=5)
        is_moldset = any([
            'PassOrFail' in df_sample.columns,
            'Cycle_Time' in df_sample.columns,
            'Barrel_Temperature' in str(df_sample.columns)
        ])
        
        if is_moldset:
            print("ğŸ” Moldset ë°ì´í„°ì…‹ í˜•ì‹ ê°ì§€")
            train_data, train_labels = load_moldset_data(
                csv_path=args.data_path,
                seq_len=args.seq_len,
                use_label=args.use_label,
                use_normal_only=args.use_normal_only
            )
            
            if train_labels is not None:
                print(f"\nğŸ“Š Label í†µê³„:")
                print(f"   - ì •ìƒ ì‹œí€€ìŠ¤: {(train_labels == 0).sum()}ê°œ")
                print(f"   - ì´ìƒ ì‹œí€€ìŠ¤: {(train_labels == 1).sum()}ê°œ")
        else:
            train_data = load_time_series_from_csv(
                args.data_path,
                seq_len=args.seq_len,
                use_moldset_format=False
            )
    else:
        print("âš ï¸  CSV íŒŒì¼ ì—†ìŒ. ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì‚¬ìš©")
        train_data = create_synthetic_time_series_data(
            n_samples=args.n_samples,
            seq_len=args.seq_len
        )
    
    # í•™ìŠµ
    model, threshold = train_model(
        train_data=train_data,
        model_type=args.model_type,
        seq_len=args.seq_len,
        epochs=args.epochs,
        batch_size=args.batch_size,
        model_save_path=args.model_save_path,
        device=device
    )
    
    # í…ŒìŠ¤íŠ¸ (ì¼ë¶€ ë°ì´í„° ì‚¬ìš©)
    test_data = train_data[:1000]  # ì²˜ìŒ 1000ê°œ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸
    test_model(model, threshold, test_data)
    
    print(f"\n{'='*80}")
    print(f"âœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    print(f"\n```python")
    print(f"from agent_system.models.anomaly_detector import AnomalyDetectionModel")
    print(f"")
    print(f"detector = AnomalyDetectionModel()")
    print(f"result = detector.detect_anomaly({{")
    print(f"    'temperature': 235.0,")
    print(f"    'pressure': 120.0,")
    print(f"    'vibration': 1.2,")
    print(f"    'cycle_time': 52.0")
    print(f"}})")
    print(f"```")
    print(f"\nğŸ“ Moldset ë°ì´í„°ì…‹ ì‚¬ìš© ì˜ˆì‹œ (ë¦¬ëˆ…ìŠ¤/A100):")
    print(f"python train_anomaly_detector.py \\")
    print(f"    --data_path dataset/moldset_labeled.csv \\")
    print(f"    --seq_len 50 \\")
    print(f"    --epochs 20 \\")
    print(f"    --use_label \\")
    print(f"    --use_normal_only \\")
    print(f"    --batch_size 128  # A100 40GB ìµœì í™” (ìë™ ê°ì§€ ì‹œ ìƒëµ ê°€ëŠ¥)")


if __name__ == "__main__":
    main()

