"""
ì œì¡° ê³µì • AI Agent LoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ (ê³ ë„í™” ë²„ì „)
A100 40GB í™˜ê²½ ìµœì í™” (QLoRA 4-bit)

ê°œì„ ì‚¬í•­:
1. CoTê°€ í¬í•¨ëœ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ (max_length í™•ì¥)
2. ë…¸ì´ì¦ˆ ë°ì´í„° ì „ì²˜ë¦¬ ê°•í™”
3. ë‹¤ì–‘í•œ Instruction í˜•ì‹ ëŒ€ì‘
4. í•™ìŠµ ì•ˆì •ì„± ê°œì„  (Gradient clipping, Warmup)
"""

import os
import torch
import json
from dataclasses import dataclass, field
from typing import Optional, List, Dict
from datasets import Dataset, load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
import numpy as np


@dataclass
class ModelArguments:
    model_name_or_path: str = field(
        default="Qwen/Qwen2.5-7B-Instruct",
        metadata={"help": "ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ. Qwen2.5-7B-Instruct ë˜ëŠ” meta-llama/Meta-Llama-3.1-8B-Instruct"}
    )
    use_4bit: bool = field(
        default=True,
        metadata={"help": "4-bit quantization ì‚¬ìš© ì—¬ë¶€ (ë©”ëª¨ë¦¬ ì ˆì•½)"}
    )
    use_flash_attention: bool = field(
        default=True,
        metadata={"help": "Flash Attention 2 ì‚¬ìš© ì—¬ë¶€ (A100ì—ì„œ ê¶Œì¥)"}
    )


@dataclass
class DataArguments:
    data_path: str = field(
        default="../1_data_generation/sft_data/manufacturing_sft_train.jsonl",
        metadata={"help": "í•™ìŠµ ë°ì´í„° ê²½ë¡œ"}
    )
    max_length: int = field(
        default=3072,  # ğŸ†• CoT í¬í•¨ìœ¼ë¡œ ì¦ê°€ (2048 â†’ 3072)
        metadata={"help": "ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"}
    )
    truncation_strategy: str = field(
        default="right",
        metadata={"help": "ê¸¸ì´ ì´ˆê³¼ ì‹œ truncation ë°©í–¥"}
    )


@dataclass
class LoraArguments:
    lora_r: int = field(
        default=64,
        metadata={"help": "LoRA attention dimension"}
    )
    lora_alpha: int = field(
        default=16,
        metadata={"help": "LoRA alpha (ìŠ¤ì¼€ì¼ë§)"}
    )
    lora_dropout: float = field(
        default=0.05,
        metadata={"help": "LoRA dropout"}
    )
    target_modules: List[str] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        metadata={"help": "LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆ"}
    )


class ManufacturingDataProcessor:
    """ì œì¡° ë°ì´í„° ì „ì²˜ë¦¬ (CoT ë° ë…¸ì´ì¦ˆ ëŒ€ì‘)"""
    
    def __init__(self, tokenizer, max_length: int = 3072):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.stats = {
            "total": 0,
            "truncated": 0,
            "avg_length": 0,
            "max_observed": 0
        }
    
    def load_jsonl(self, data_path: str) -> Dataset:
        """JSONL íŒŒì¼ ë¡œë“œ"""
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line))
        return Dataset.from_list(data)
    
    def clean_input(self, text: str) -> str:
        """ğŸ†• ë…¸ì´ì¦ˆ ì œê±° (ì„ íƒì ) - ë„ˆë¬´ ê³¼í•œ ë…¸ì´ì¦ˆë§Œ ì •ë¦¬"""
        # ì—°ì†ëœ ë¹ˆ ì¤„ ì œê±°
        import re
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
        return text.strip()
    
    def format_prompt_qwen(self, example: Dict) -> str:
        """Qwen2.5 ChatML í˜•ì‹"""
        prompt = f"""<|im_start|>system
{example['instruction']}<|im_end|>
<|im_start|>user
{example['input']}<|im_end|>
<|im_start|>assistant
{example['output']}<|im_end|>"""
        return prompt
    
    def format_prompt_llama(self, example: Dict) -> str:
        """Llama 3.1 í˜•ì‹"""
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{example['instruction']}<|eot_id|><|start_header_id|>user<|end_header_id|>

{example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{example['output']}<|eot_id|>"""
        return prompt
    
    def format_prompt(self, example: Dict) -> str:
        """ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í¬ë§· ì„ íƒ"""
        model_name = self.tokenizer.name_or_path.lower()
        
        if "llama" in model_name:
            return self.format_prompt_llama(example)
        else:  # Qwen ë° ê¸°íƒ€
            return self.format_prompt_qwen(example)
    
    def tokenize_function(self, examples: List[Dict]) -> Dict:
        """í† í¬ë‚˜ì´ì§• with í†µê³„ ìˆ˜ì§‘"""
        prompts = []
        for ex in examples:
            # ì…ë ¥ ì •ë¦¬ (ì„ íƒì )
            cleaned_input = self.clean_input(ex['input'])
            example_dict = {
                'instruction': ex['instruction'],
                'input': cleaned_input,
                'output': ex['output']
            }
            prompts.append(self.format_prompt(example_dict))
        
        # í† í¬ë‚˜ì´ì§•
        tokenized = self.tokenizer(
            prompts,
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors=None,
            add_special_tokens=True  # ğŸ†• ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
        )
        
        # í†µê³„ ìˆ˜ì§‘
        for input_ids in tokenized["input_ids"]:
            length = len(input_ids)
            self.stats["total"] += 1
            self.stats["avg_length"] += length
            self.stats["max_observed"] = max(self.stats["max_observed"], length)
            if length >= self.max_length:
                self.stats["truncated"] += 1
        
        # Labels = input_ids (causal LM)
        tokenized["labels"] = [ids.copy() for ids in tokenized["input_ids"]]
        
        return tokenized
    
    def prepare_dataset(self, data_path: str) -> Dataset:
        """ì „ì²´ ë°ì´í„°ì…‹ ì¤€ë¹„"""
        dataset = self.load_jsonl(data_path)
        
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ ìƒ˜í”Œ")
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ í† í¬ë‚˜ì´ì§•
        tokenized_dataset = dataset.map(
            lambda batch: self.tokenize_function(
                [{"instruction": inst, "input": inp, "output": out} 
                 for inst, inp, out in zip(batch["instruction"], batch["input"], batch["output"])]
            ),
            batched=True,
            batch_size=10,
            remove_columns=dataset.column_names,
            desc="í† í¬ë‚˜ì´ì§• ì¤‘..."
        )
        
        # í†µê³„ ì¶œë ¥
        if self.stats["total"] > 0:
            avg_len = self.stats["avg_length"] / self.stats["total"]
            trunc_pct = (self.stats["truncated"] / self.stats["total"]) * 100
            
            print(f"\nğŸ“Š í† í¬ë‚˜ì´ì§• í†µê³„:")
            print(f"   - í‰ê·  ê¸¸ì´: {avg_len:.0f} í† í°")
            print(f"   - ìµœëŒ€ ê¸¸ì´: {self.stats['max_observed']} í† í°")
            print(f"   - Truncation ë°œìƒ: {self.stats['truncated']}ê°œ ({trunc_pct:.1f}%)")
            
            if trunc_pct > 20:
                print(f"\nâš ï¸  ê²½ê³ : {trunc_pct:.0f}%ì˜ ìƒ˜í”Œì´ ì˜ë ¸ìŠµë‹ˆë‹¤!")
                print(f"   â†’ max_lengthë¥¼ {int(self.stats['max_observed'] * 1.1)}ë¡œ ì¦ê°€ ê¶Œì¥")
        
        return tokenized_dataset
    
    def print_sample(self, dataset: Dataset, idx: int = 0):
        """ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)"""
        sample = dataset[idx]
        decoded = self.tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“„ ìƒ˜í”Œ #{idx} (ê¸¸ì´: {len(sample['input_ids'])} í† í°)")
        print(f"{'='*80}")
        print(decoded[:1000])
        if len(decoded) > 1000:
            print(f"\n... (ì¤‘ëµ, ì´ {len(decoded)}ì) ...\n")
            print(decoded[-500:])
        print(f"{'='*80}\n")


def setup_model_and_tokenizer(model_args: ModelArguments):
    """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì •"""
    
    # BitsAndBytes ì„¤ì • (4-bit quantization)
    if model_args.use_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
        print("âœ… 4-bit Quantization í™œì„±í™”")
    else:
        bnb_config = None
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        trust_remote_code=True,
        padding_side="right",
        use_fast=True  # ğŸ†• Fast tokenizer ì‚¬ìš©
    )
    
    # íŒ¨ë”© í† í° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    print(f"ğŸ“ í† í¬ë‚˜ì´ì € ì •ë³´:")
    print(f"   - Vocab size: {len(tokenizer)}")
    print(f"   - PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    print(f"   - EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        attn_implementation="sdpa"
    )
    
    # Gradient checkpointing (ë©”ëª¨ë¦¬ ì ˆì•½)
    model.config.use_cache = False
    model.config.pretraining_tp = 1
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_args.model_name_or_path}")
    print(f"   - Attention: SDPA (PyTorch Native)")
    print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    
    return model, tokenizer


def setup_lora(model, lora_args: LoraArguments):
    """LoRA ì„¤ì •"""
    
    # k-bit í•™ìŠµ ì¤€ë¹„
    model = prepare_model_for_kbit_training(model)
    
    # LoRA êµ¬ì„±
    lora_config = LoraConfig(
        r=lora_args.lora_r,
        lora_alpha=lora_args.lora_alpha,
        target_modules=lora_args.target_modules,
        lora_dropout=lora_args.lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    print(f"\nâœ… LoRA ì„¤ì • ì™„ë£Œ:")
    print(f"   - Rank: {lora_args.lora_r}")
    print(f"   - Alpha: {lora_args.lora_alpha}")
    print(f"   - Target modules: {len(lora_args.target_modules)}ê°œ")
    
    return model


def compute_metrics(eval_pred):
    """ğŸ†• í‰ê°€ ì§€í‘œ ê³„ì‚°"""
    predictions, labels = eval_pred
    
    # Perplexity ê³„ì‚° (ê°„ë‹¨ ë²„ì „)
    # ì‹¤ì œë¡œëŠ” lossë¥¼ ì‚¬ìš©í•˜ëŠ” ê²Œ ë” ì •í™•
    return {}


def main():
    print("=" * 80)
    print("ì œì¡° ê³µì • AI Agent LoRA íŒŒì¸íŠœë‹ (ê³ ë„í™” ë²„ì „)")
    print("A100 40GB ìµœì í™” + CoT + ë…¸ì´ì¦ˆ ëŒ€ì‘")
    print("=" * 80)
    
    # ì¸ì ì„¤ì •
    model_args = ModelArguments()
    data_args = DataArguments()
    lora_args = LoraArguments()
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        raise RuntimeError("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GPU í™˜ê²½ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    print(f"\nğŸš€ GPU ì •ë³´:")
    print(f"   - Device: {torch.cuda.get_device_name(0)}")
    print(f"   - VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    print(f"   - CUDA: {torch.version.cuda}")
    
    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\n" + "=" * 80)
    print("1ï¸âƒ£ ëª¨ë¸ ë¡œë”© ì¤‘...")
    print("=" * 80)
    model, tokenizer = setup_model_and_tokenizer(model_args)
    
    # 2. LoRA ì„¤ì •
    print("\n" + "=" * 80)
    print("2ï¸âƒ£ LoRA ì„¤ì • ì¤‘...")
    print("=" * 80)
    model = setup_lora(model, lora_args)
    
    # 3. ë°ì´í„° ì¤€ë¹„
    print("\n" + "=" * 80)
    print("3ï¸âƒ£ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    print("=" * 80)
    processor = ManufacturingDataProcessor(tokenizer, max_length=data_args.max_length)
    train_dataset = processor.prepare_dataset(data_args.data_path)
    
    # ìƒ˜í”Œ ì¶œë ¥ (ì²« ë²ˆì§¸ ìƒ˜í”Œ)
    processor.print_sample(train_dataset, idx=0)
    
    # Train/Eval ë¶„ë¦¬ (90:10)
    split_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = split_dataset["train"]
    eval_dataset = split_dataset["test"]
    
    print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"   - í•™ìŠµ ìƒ˜í”Œ: {len(train_dataset)}ê°œ")
    print(f"   - ê²€ì¦ ìƒ˜í”Œ: {len(eval_dataset)}ê°œ")
    
    # 4. í•™ìŠµ ì„¤ì •
    print("\n" + "=" * 80)
    print("4ï¸âƒ£ í•™ìŠµ ì„¤ì •...")
    print("=" * 80)
    
    output_dir = "./manufacturing_lora_output"
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=2,  # ğŸ†• ê¸´ ì‹œí€€ìŠ¤ ëŒ€ì‘ (4â†’2)
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=8,  # ğŸ†• Effective batch size = 16 ìœ ì§€
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        warmup_ratio=0.05,
        max_grad_norm=1.0,  # ğŸ†• Gradient clipping ì¶”ê°€
        logging_steps=5,
        save_strategy="epoch",
        eval_strategy="epoch",
        bf16=True,
        gradient_checkpointing=True,
        optim="paged_adamw_8bit",
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="loss",
        report_to="none",
        ddp_find_unused_parameters=False,
        dataloader_num_workers=4,  # ğŸ†• ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”
        dataloader_pin_memory=True,
        group_by_length=False,  # ğŸ†• ê¸¸ì´ë³„ ê·¸ë£¹í™” ë¹„í™œì„±í™” (ë‹¤ì–‘ì„± í™•ë³´)
    )
    
    # Data Collator
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        pad_to_multiple_of=8,
        return_tensors="pt",
        padding=True
    )
    
    # 5. Trainer ì´ˆê¸°í™”
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )
    
    # 6. í•™ìŠµ ì‹œì‘
    print("\n" + "=" * 80)
    print("5ï¸âƒ£ í•™ìŠµ ì‹œì‘! ğŸš€")
    print("=" * 80)
    print(f"   - Epochs: {training_args.num_train_epochs}")
    print(f"   - Batch size: {training_args.per_device_train_batch_size}")
    print(f"   - Gradient accumulation: {training_args.gradient_accumulation_steps}")
    print(f"   - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    print(f"   - Learning rate: {training_args.learning_rate}")
    print(f"   - Max sequence length: {data_args.max_length}")
    print(f"   - Total steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}")
    print()
    
    # í•™ìŠµ ì‹¤í–‰
    train_result = trainer.train()
    
    # 7. ëª¨ë¸ ì €ì¥
    print("\n" + "=" * 80)
    print("6ï¸âƒ£ í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ì¤‘...")
    print("=" * 80)
    
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # í•™ìŠµ ê²°ê³¼ ì €ì¥
    with open(os.path.join(output_dir, "training_results.json"), "w") as f:
        json.dump(train_result.metrics, f, indent=2)
    
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")
    print(f"   - LoRA ì–´ëŒ‘í„°: {output_dir}/adapter_model.safetensors")
    print(f"   - ì„¤ì • íŒŒì¼: {output_dir}/adapter_config.json")
    print(f"   - í•™ìŠµ ê²°ê³¼: {output_dir}/training_results.json")
    
    # ìµœì¢… Loss ì¶œë ¥
    print(f"\nğŸ“Š ìµœì¢… í•™ìŠµ ì§€í‘œ:")
    print(f"   - Train Loss: {train_result.metrics.get('train_loss', 'N/A'):.4f}")
    print(f"   - Learning Rate: {train_result.metrics.get('train_runtime', 0) / 3600:.2f} hours")
    
    # 8. ì¶”ë¡  í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 80)
    print("7ï¸âƒ£ ì¶”ë¡  í…ŒìŠ¤íŠ¸ (CoT ìƒì„± í™•ì¸)")
    print("=" * 80)
    
    test_input = """[ê³µì • ì´ìƒ ì´ë²¤íŠ¸]
ì„¤ë¹„: ì‚¬ì¶œê¸°-3í˜¸ê¸°
ë°œìƒì‹œê°: 2024-12-08 09:15:00
ì´ìƒìœ í˜•: ì˜¨ë„ ì´ìƒ

[ì„¼ì„œ ë°ì´í„°]
- ì‹¤ë¦°ë” ì˜¨ë„: 235Â°C (ì •ìƒ: 200Â°C)
- ì„ê³„ê°’: ì„¤ì •ê°’ ëŒ€ë¹„ Â±15Â°C
- ì¦ìƒ: ì œí’ˆ ë³€í˜•, ì¹˜ìˆ˜ ë¶ˆëŸ‰

[RAG ê²€ìƒ‰ ê²°ê³¼]
[ê³¼ê±° ì´ë ¥ #2023-11-20] ë™ì¼ ì¦ìƒìœ¼ë¡œ íˆí„° ê³ ì¥ í™•ì¸ë¨. êµì²´ í›„ ì •ìƒí™”.
[ì„¤ë¹„ ë§¤ë‰´ì–¼ 3.2ì ˆ] ì˜¨ë„ ì´ìƒ ë°œìƒ ì‹œ íˆí„° ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„± ë†’ìŒ.

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›ì¸ ë¶„ì„ê³¼ ì¡°ì¹˜ ê°€ì´ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”."""
    
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì œì¡° í˜„ì¥ì˜ ì „ë¬¸ ì„¤ë¹„ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤."},
        {"role": "user", "content": test_input}
    ]
    
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    print(f"   - Input ê¸¸ì´: {inputs['input_ids'].shape[1]} í† í°")
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=1024,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        repetition_penalty=1.1  # ğŸ†• ë°˜ë³µ ë°©ì§€
    )
    
    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    
    print("\n[í…ŒìŠ¤íŠ¸ ì‘ë‹µ]")
    print("-" * 80)
    print(response[:800])
    if len(response) > 800:
        print("\n... (ì¤‘ëµ) ...\n")
        print(response[-200:])
    print("-" * 80)
    
    # CoT í¬í•¨ ì—¬ë¶€ í™•ì¸
    has_cot = "ìƒí™© ë¶„ì„" in response or "ì¶”ë¡  ê³¼ì •" in response or "ë‹¨ê³„" in response
    print(f"\n{'âœ…' if has_cot else 'âš ï¸ '} CoT ì¶”ë¡  ê³¼ì • {'í¬í•¨ë¨' if has_cot else 'ë¯¸í¬í•¨ (ì¶”ê°€ í•™ìŠµ í•„ìš”)'}")
    
    print("\n" + "=" * 80)
    print("âœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("=" * 80)
    print(f"\në‹¤ìŒ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
    print(f"\n```python")
    print(f"from peft import PeftModel")
    print(f"from transformers import AutoModelForCausalLM, AutoTokenizer")
    print(f"")
    print(f"base_model = AutoModelForCausalLM.from_pretrained('{model_args.model_name_or_path}')")
    print(f"model = PeftModel.from_pretrained(base_model, '{output_dir}')")
    print(f"tokenizer = AutoTokenizer.from_pretrained('{output_dir}')")
    print(f"```")
    print("=" * 80)


if __name__ == "__main__":
    main()