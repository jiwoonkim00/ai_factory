"""
4_agent_system/models/rag_system.py
RAG ì‹œìŠ¤í…œ - ê³¼ê±° ì´ë ¥, ë§¤ë‰´ì–¼ ê²€ìƒ‰
"""


from typing import List, Dict, Optional
import torch

try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_core.documents import Document
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    print(f"âš ï¸  LangChain ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("   í•„ìš” íŒ¨í‚¤ì§€: pip install langchain langchain-community langchain-text-splitters faiss-cpu sentence-transformers")

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("âš ï¸  ChromaDB ë¯¸ì„¤ì¹˜: pip install chromadb")


class RAGSystem:
    """RAG ì‹œìŠ¤í…œ - ê³¼ê±° ì´ë ¥, ë§¤ë‰´ì–¼ ê²€ìƒ‰"""
    
    def __init__(self, knowledge_base_path: str = None, use_chromadb: bool = False):
        """
        Args:
            knowledge_base_path: ì§€ì‹ ë² ì´ìŠ¤ ê²½ë¡œ (Noneì´ë©´ ìë™ íƒì§€)
            use_chromadb: ChromaDB ì‚¬ìš© ì—¬ë¶€ (Trueë©´ ChromaDB, Falseë©´ FAISS)
        """
        # ê²½ë¡œ ìë™ ì„¤ì •
        if knowledge_base_path is None:
            try:
                from utils.config import KNOWLEDGE_BASE_PATH, VECTOR_DB_PATH
                knowledge_base_path = str(KNOWLEDGE_BASE_PATH)
                self.vector_db_path = str(VECTOR_DB_PATH)
            except ImportError:
                # config ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
                
                project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                knowledge_base_path = os.path.join(project_root, "3_knowledge_base", "knowledge_base")
                self.vector_db_path = os.path.join(project_root, "3_knowledge_base", "vector_db")
        else:
            # knowledge_base_pathê°€ ì „ë‹¬ëœ ê²½ìš°ì—ë„ vector_db_path ì„¤ì •
            print(f"ğŸ” knowledge_base_pathê°€ ì „ë‹¬ë¨: {knowledge_base_path}")
            try:
                from utils.config import VECTOR_DB_PATH
                print(f"ğŸ” config.VECTOR_DB_PATH = {VECTOR_DB_PATH}")
                self.vector_db_path = str(VECTOR_DB_PATH)
                print(f"ğŸ” ì„¤ì •ëœ vector_db_path (from config): {self.vector_db_path}")
            except ImportError as e:
                # config ì—†ìœ¼ë©´ knowledge_base_path ê¸°ì¤€ìœ¼ë¡œ ìë™ ì„¤ì •
                print(f"ğŸ” config import ì‹¤íŒ¨: {e}, ìë™ ê³„ì‚° ì‚¬ìš©")
                kb_parent = os.path.dirname(knowledge_base_path)
                self.vector_db_path = os.path.join(kb_parent, "vector_db")
                print(f"ğŸ” ì„¤ì •ëœ vector_db_path (ìë™): {self.vector_db_path}")
        
        self.knowledge_base_path = knowledge_base_path
        self.use_chromadb = use_chromadb and CHROMADB_AVAILABLE
        self.embeddings = None
        self.embedding_model = None  # sentence-transformers ëª¨ë¸ (ChromaDBìš©)
        self.vectorstore = None
        self.chroma_collection = None
        self.documents = []
        self.is_loaded = False
        
        if not LANGCHAIN_AVAILABLE:
            print("âš ï¸  LangChain ì—†ìŒ. RAG ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ“š RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        self._initialize_embeddings()
        self._load_knowledge_base()
    
    def _initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if not LANGCHAIN_AVAILABLE:
            return
        
        try:
            # config íŒŒì¼ì—ì„œ ì„¤ì • ì½ê¸°
            embedding_model = "BAAI/bge-m3"
            try:
                from utils.config import RAG_CONFIG
                embedding_model = RAG_CONFIG.get("embedding_model", embedding_model)
            except:
                pass
            
            self.embeddings = HuggingFaceEmbeddings(
                model_name=embedding_model,
                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
            )
            print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({embedding_model})")
        except Exception as e:
            print(f"âš ï¸  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # Fallback
            try:
                fallback_model = "sentence-transformers/all-MiniLM-L6-v2"
                try:
                    from utils.config import RAG_CONFIG
                    fallback_model = RAG_CONFIG.get("fallback_embedding_model", fallback_model)
                except:
                    pass
                
                self.embeddings = HuggingFaceEmbeddings(model_name=fallback_model)
                print(f"âœ… Fallback ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {fallback_model}")
            except Exception as e2:
                print(f"âŒ Fallback ì„ë² ë”© ëª¨ë¸ë„ ë¡œë“œ ì‹¤íŒ¨: {e2}")
                self.embeddings = None
    
    def _load_knowledge_base(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ë° ë²¡í„°í™”"""
        os.makedirs(self.knowledge_base_path, exist_ok=True)
        
        # ë””ë²„ê¹… ì •ë³´
        print(f"ğŸ” RAG ë¡œë”© ì„¤ì •:")
        print(f"   - use_chromadb: {self.use_chromadb}")
        print(f"   - vector_db_path: {self.vector_db_path}")
        print(f"   - CHROMADB_AVAILABLE: {CHROMADB_AVAILABLE}")
        
        # ChromaDB ìš°ì„  ì‹œë„ (LangChain ë¶ˆí•„ìš”)
        if self.use_chromadb and self.vector_db_path and CHROMADB_AVAILABLE:
            print(f"ğŸ”„ ChromaDB ë¡œë“œ ì‹œë„ ì¤‘: {self.vector_db_path}")
            try:
                self._load_from_chromadb()
                if self.is_loaded:
                    print(f"âœ… ChromaDBì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë¨")
                    return
                else:
                    print(f"âš ï¸  ChromaDB ë¡œë“œí–ˆì§€ë§Œ is_loaded=False")
            except Exception as e:
                print(f"âš ï¸  ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        elif self.use_chromadb:
            if not CHROMADB_AVAILABLE:
                print(f"âš ï¸  ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ (pip install chromadb)")
            if not self.vector_db_path:
                print(f"âš ï¸  vector_db_pathê°€ Noneì„")
        
        # FAISS ì‚¬ìš© ì‹œì—ëŠ” LangChain í•„ìš”
        if not LANGCHAIN_AVAILABLE or self.embeddings is None:
            print("âš ï¸  LangChain ì—†ìŒ. ChromaDB ë˜ëŠ” ìƒ˜í”Œ ë¬¸ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            if not self.is_loaded:
                self._load_sample_documents_simple()
            return
        
        # íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
        try:
            self._load_from_files()
            if self.is_loaded:
                return
        except Exception as e:
            print(f"âš ï¸  íŒŒì¼ì—ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ìƒ˜í”Œ ë¬¸ì„œë¡œ í´ë°±
        print("âš ï¸  ì§€ì‹ ë² ì´ìŠ¤ íŒŒì¼ ì—†ìŒ. ìƒ˜í”Œ ë¬¸ì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        self._load_sample_documents()
    
    def _load_from_chromadb(self):
        """ChromaDBì—ì„œ ë¡œë“œ"""
        if not CHROMADB_AVAILABLE or not self.vector_db_path:
            print(f"âš ï¸  ChromaDB ì¡°ê±´ ë¯¸ì¶©ì¡±: AVAILABLE={CHROMADB_AVAILABLE}, path={self.vector_db_path}")
            return
        
        try:
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
            print(f"ğŸ”— ChromaDB ì—°ê²° ì¤‘: {self.vector_db_path}")
            client = chromadb.PersistentClient(
                path=str(self.vector_db_path),
                settings=Settings(allow_reset=False)
            )
            
            # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
            collections = client.list_collections()
            collection_names = [c.name for c in collections]
            print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {collection_names}")
            
            if "manufacturing_kb" not in collection_names:
                print(f"âš ï¸  'manufacturing_kb' ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"   ChromaDBë¥¼ êµ¬ì¶•í•˜ë ¤ë©´: cd 3_knowledge_base && python setup_rag.py")
                return
            
            # ì»¬ë ‰ì…˜ ë¡œë“œ
            collection = client.get_collection("manufacturing_kb")
            self.chroma_collection = collection
            
            # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
            count = collection.count()
            print(f"ğŸ“š ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜: {count}ê°œ")
            
            if count == 0:
                print(f"âš ï¸  ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return
            
            # ChromaDB ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (sentence-transformers ì§ì ‘ ì‚¬ìš©)
            try:
                from sentence_transformers import SentenceTransformer
                
                # ì—¬ëŸ¬ ëª¨ë¸ ì‹œë„ (fallback)
                models_to_try = [
                    "sentence-transformers/all-MiniLM-L6-v2",  # ê°€ë³ê³  ì•ˆì •ì 
                    "BAAI/bge-m3",  # ê³ ì„±ëŠ¥
                ]
                
                for embedding_model in models_to_try:
                    try:
                        print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œë„: {embedding_model}")
                        self.embedding_model = SentenceTransformer(embedding_model)
                        print(f"âœ… ChromaDBìš© ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {embedding_model}")
                        break
                    except Exception as e:
                        print(f"âš ï¸  {embedding_model} ë¡œë“œ ì‹¤íŒ¨: {e}")
                        continue
                
                if self.embedding_model is None:
                    print(f"âš ï¸  ëª¨ë“  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ChromaDB ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©")
                    
            except Exception as e:
                print(f"âš ï¸  sentence-transformers ë¡œë“œ ì‹¤íŒ¨: {e}")
                print(f"   pip install sentence-transformers í•„ìš”")
                self.embedding_model = None
            
            # ì„±ê³µ
            self.is_loaded = True
            print(f"âœ… ChromaDBì—ì„œ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ ({count}ê°œ ë¬¸ì„œ)")
            
        except Exception as e:
            print(f"âŒ ChromaDB ì—°ê²° ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            self.is_loaded = False
    
    def _load_from_files(self):
        """íŒŒì¼ì—ì„œ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        from pathlib import Path
        kb_path = Path(self.knowledge_base_path)
        
        if not kb_path.exists():
            return
        
        # ì‹¤ì œ íŒŒì¼ ë¡œë“œ ë¡œì§ì€ setup_rag.py ì°¸ê³ 
        # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ìƒ˜í”Œë§Œ ì‚¬ìš©
        pass
    
    def _load_sample_documents(self):
        """ìƒ˜í”Œ ë¬¸ì„œ ë¡œë“œ (í´ë°±)"""
        if not LANGCHAIN_AVAILABLE or self.embeddings is None:
            return
        
        sample_docs = self._create_sample_documents()
        
        # ë¬¸ì„œ ë¶„í• 
        chunk_size = 500
        chunk_overlap = 50
        try:
            from utils.config import RAG_CONFIG
            chunk_size = RAG_CONFIG.get("chunk_size", chunk_size)
            chunk_overlap = RAG_CONFIG.get("chunk_overlap", chunk_overlap)
        except:
            pass
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        self.documents = []
        for doc_dict in sample_docs:
            doc = Document(
                page_content=doc_dict['content'],
                metadata=doc_dict['metadata']
            )
            self.documents.append(doc)
        
        splits = text_splitter.split_documents(self.documents)
        
        # Vector DB ìƒì„±
        if len(splits) > 0:
            self.vectorstore = FAISS.from_documents(splits, self.embeddings)
            self.is_loaded = True
            print(f"âœ… ìƒ˜í”Œ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ ({len(splits)}ê°œ ì²­í¬)")
        else:
            print("âš ï¸  ì§€ì‹ ë² ì´ìŠ¤ ë¬¸ì„œ ì—†ìŒ")
    
    def _load_sample_documents_simple(self):
        """LangChain ì—†ì´ ê°„ë‹¨í•œ ìƒ˜í”Œ ë¬¸ì„œ ë¡œë“œ"""
        self.documents = self._create_sample_documents()
        self.is_loaded = True
        print(f"âœ… ìƒ˜í”Œ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ ({len(self.documents)}ê°œ ë¬¸ì„œ, LangChain ì—†ìŒ)")
    
    def _create_sample_documents(self) -> List[Dict]:
        """ìƒ˜í”Œ ì§€ì‹ ë² ì´ìŠ¤ ë¬¸ì„œ ìƒì„±"""
        return [
            {
                'content': """
                [ê³¼ê±° ì´ë ¥ #2023-08-15]
                ì„¤ë¹„: ì‚¬ì¶œê¸°-2í˜¸ê¸°
                ì¦ìƒ: ì‹¤ë¦°ë” ì˜¨ë„ ê¸‰ìƒìŠ¹ (235Â°C)
                ì›ì¸: íˆí„° ì½”ì¼ ë‹¨ì„ 
                ì¡°ì¹˜: íˆí„° êµì²´ í›„ ì •ìƒí™”
                ì†Œìš”ì‹œê°„: 4ì‹œê°„
                """,
                'metadata': {
                    'type': 'ê³¼ê±°_ì´ë ¥',
                    'equipment': 'ì‚¬ì¶œê¸°',
                    'anomaly': 'ì˜¨ë„_ì´ìƒ'
                }
            },
            {
                'content': """
                [ì„¤ë¹„ ë§¤ë‰´ì–¼ 3.2ì ˆ - ì˜¨ë„ ê´€ë¦¬]
                ì‹¤ë¦°ë” ì˜¨ë„ê°€ ì„¤ì •ê°’ Â±15Â°Cë¥¼ ë²—ì–´ë‚  ê²½ìš°:
                1. íˆí„° ì €í•­ê°’ ì¸¡ì • (ì •ìƒ: 30~35Î©)
                2. ì—´ì „ëŒ€ ì„¼ì„œ ì ê²€
                3. ì˜¨ë„ ì œì–´ê¸° íŒŒë¼ë¯¸í„° í™•ì¸
                ê¸´ê¸‰ ì¡°ì¹˜: ì¦‰ì‹œ ì„¤ë¹„ ì •ì§€
                """,
                'metadata': {
                    'type': 'ë§¤ë‰´ì–¼',
                    'equipment': 'ì‚¬ì¶œê¸°',
                    'section': 'ì˜¨ë„_ê´€ë¦¬'
                }
            },
            {
                'content': """
                [Trouble Shooting Guide]
                ì••ë ¥ ì´ìƒ ë°œìƒ ì‹œ ì ê²€ ìˆœì„œ:
                1. ìœ ì••íŒí”„ ì••ë ¥ ê²Œì´ì§€ í™•ì¸
                2. ì‹¤ë¦°ë” ì”° ëˆ„ìœ  ì ê²€
                3. ë°°ê´€ ì—°ê²°ë¶€ ì ê²€
                4. ì••ë ¥ ì„¼ì„œ êµì •
                ì£¼ì˜: ì••ë ¥ì´ ì •ìƒ ë²”ìœ„ì˜ 70% ì´í•˜ë©´ ì¦‰ì‹œ ì •ì§€
                """,
                'metadata': {
                    'type': 'Trouble_Shooting',
                    'equipment': 'ì „ì²´',
                    'anomaly': 'ì••ë ¥_ì´ìƒ'
                }
            },
            {
                'content': """
                [ì •ë¹„ ì´ë ¥ DB]
                ìµœê·¼ 6ê°œì›” ì§„ë™ ì´ìƒ ì¼€ì´ìŠ¤:
                - ë² ì–´ë§ ë§ˆëª¨: 5ê±´ (í‰ê·  ë³µêµ¬ ì‹œê°„ 6ì‹œê°„)
                - êµ¬ë™ë¶€ ì–¸ë°¸ëŸ°ìŠ¤: 3ê±´ (í‰ê·  ë³µêµ¬ ì‹œê°„ 4ì‹œê°„)
                - ì²´ê²°ë¶€ í’€ë¦¼: 2ê±´ (í‰ê·  ë³µêµ¬ ì‹œê°„ 2ì‹œê°„)
                ì˜ˆë°© ì¡°ì¹˜: ì›” 1íšŒ ì§„ë™ ì¸¡ì • ë° ë² ì–´ë§ ê·¸ë¦¬ìŠ¤ ì£¼ì…
                """,
                'metadata': {
                    'type': 'ì •ë¹„_ì´ë ¥',
                    'anomaly': 'ì§„ë™_ì´ìƒ',
                    'period': 'ìµœê·¼_6ê°œì›”'
                }
            },
            {
                'content': """
                [8D Report ì˜ˆì‹œ #2024-03-20]
                D1: íŒ€ êµ¬ì„± - ìƒì‚°ê¸°ìˆ íŒ€, í’ˆì§ˆíŒ€
                D2: ë¬¸ì œ ì •ì˜ - ì‚¬ì¶œê¸° 3í˜¸ê¸° ì˜¨ë„ ì´ìƒ
                D3: ì„ì‹œ ì¡°ì¹˜ - ì„¤ë¹„ ì •ì§€, ìƒì‚°í’ˆ ê²©ë¦¬
                D4: ê·¼ë³¸ ì›ì¸ - ëƒ‰ê°ìˆ˜ ìˆœí™˜ íŒí”„ ê³ ì¥
                D5: ì˜êµ¬ ëŒ€ì±… - íŒí”„ êµì²´, ì˜ˆë¹„í’ˆ í™•ë³´
                D6: ì‹¤í–‰ ë° ê²€ì¦ - 48ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì •ìƒ
                D7: ì¬ë°œ ë°©ì§€ - PM ì£¼ê¸° ì¡°ì •, ì„¼ì„œ ì¶”ê°€
                """,
                'metadata': {
                    'type': '8D_Report',
                    'equipment': 'ì‚¬ì¶œê¸°',
                    'date': '2024-03-20'
                }
            }
        ]
    
    def search(self, query: str, k: int = None) -> List[Dict]:
        """
        ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©)
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if k is None:
            k = 3  # ê¸°ë³¸ê°’
            try:
                from utils.config import RAG_CONFIG
                k = RAG_CONFIG.get("search_k", k)
            except:
                pass
        
        # ChromaDB ì‚¬ìš©
        if self.use_chromadb and self.chroma_collection is not None:
            try:
                # ì„ë² ë”© ìƒì„± í›„ ê²€ìƒ‰
                if self.embedding_model is not None:
                    query_embedding = self.embedding_model.encode([query]).tolist()
                    results = self.chroma_collection.query(
                        query_embeddings=query_embedding,
                        n_results=k
                    )
                else:
                    # ì„ë² ë”© ëª¨ë¸ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì§ì ‘ ì‚¬ìš© (ChromaDB ê¸°ë³¸ ì„ë² ë”©)
                    results = self.chroma_collection.query(
                        query_texts=[query],
                        n_results=k
                    )
                
                retrieved = []
                if results['documents'] and len(results['documents'][0]) > 0:
                    for i, (doc, metadata, distance) in enumerate(zip(
                        results['documents'][0],
                        results['metadatas'][0] if results['metadatas'] else [{}] * len(results['documents'][0]),
                        results['distances'][0] if results.get('distances') else [0] * len(results['documents'][0])
                    )):
                        # cosine distanceë¥¼ similarityë¡œ ë³€í™˜ (0=ë™ì¼, 2=ì™„ì „ë°˜ëŒ€)
                        similarity = 1 - (distance / 2) if distance else 0.9 - (i * 0.1)
                        retrieved.append({
                            'content': doc,
                            'metadata': metadata,
                            'similarity': max(0, min(1, similarity))  # 0~1 ë²”ìœ„ë¡œ ì œí•œ
                        })
                return retrieved
            except Exception as e:
                print(f"âš ï¸  ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        # FAISS ì‚¬ìš©
        if self.vectorstore is not None:
            try:
                results = self.vectorstore.similarity_search_with_score(query, k=k)
                
                retrieved = []
                for doc, score in results:
                    retrieved.append({
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'similarity': float(1 - score) if score <= 1.0 else float(1 / (1 + score))  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    })
                
                return retrieved
            except Exception as e:
                print(f"âš ï¸  FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë¬¸ì„œì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ (í´ë°±)
        if self.documents:
            print("âš ï¸  ë²¡í„° ê²€ìƒ‰ ë¶ˆê°€. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            retrieved = []
            query_lower = query.lower()
            for doc in self.documents[:k]:
                content = doc.get('content', '')
                if any(keyword in content.lower() for keyword in query_lower.split()):
                    retrieved.append({
                        'content': content,
                        'metadata': doc.get('metadata', {}),
                        'similarity': 0.5  # í‚¤ì›Œë“œ ë§¤ì¹­ì´ë¯€ë¡œ ë‚®ì€ ìœ ì‚¬ë„
                    })
            return retrieved[:k]
        
        # ì™„ì „ ì‹¤íŒ¨
        print("âš ï¸  ê²€ìƒ‰ ì‹œìŠ¤í…œ ì—†ìŒ. ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return []