"""
4_agent_system/models/lora_inference.py
LoRA íŒŒì¸íŠœë‹ ëª¨ë¸ ì¶”ë¡  ì—”ì§„
"""

import os
import torch
from typing import Optional
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ============================================================================
# LoRA ì¶”ë¡  ì—”ì§„
# ============================================================================

class LoRAInferenceEngine:
    """LoRA íŒŒì¸íŠœë‹ ëª¨ë¸ ì¶”ë¡  ì—”ì§„"""
    
    def __init__(self, 
                 base_model_path: str = "Qwen/Qwen2.5-7B-Instruct",
                 lora_adapter_path: str = None):
        """
        Args:
            base_model_path: Base ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” HuggingFace ëª¨ë¸ëª…
            lora_adapter_path: LoRA ì–´ëŒ‘í„° ê²½ë¡œ (Noneì´ë©´ ìë™ íƒì§€)
        """
        self.base_model_path = base_model_path
        
        # ê²½ë¡œ ìë™ ì„¤ì •
        if lora_adapter_path is None:
            from ..utils.config import LORA_MODEL_PATH
            lora_adapter_path = str(LORA_MODEL_PATH)
        
        self.lora_adapter_path = lora_adapter_path
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        
        print(f"ğŸ¤– LoRA ëª¨ë¸ ë¡œë”© ì¤‘...")
        self._load_model()
    
    def _load_model(self):
        """Base ëª¨ë¸ + LoRA ì–´ëŒ‘í„° ë¡œë“œ"""
        try:
            # í† í¬ë‚˜ì´ì €
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_path,
                trust_remote_code=True
            )
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
            # Base ëª¨ë¸
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # LoRA ì–´ëŒ‘í„° (í•™ìŠµ ì™„ë£Œ í›„)
            adapter_exists = os.path.exists(self.lora_adapter_path) and \
                           os.path.exists(os.path.join(self.lora_adapter_path, "adapter_config.json"))
            
            if adapter_exists:
                try:
                    self.model = PeftModel.from_pretrained(
                        self.model, 
                        self.lora_adapter_path
                    )
                    print(f"âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ: {self.lora_adapter_path}")
                except Exception as e:
                    print(f"âš ï¸  LoRA ì–´ëŒ‘í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                    print(f"   Base ëª¨ë¸ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                print(f"âš ï¸  LoRA ì–´ëŒ‘í„° ì—†ìŒ. Base ëª¨ë¸ë§Œ ì‚¬ìš©: {self.base_model_path}")
                print(f"   ê²½ë¡œ í™•ì¸: {self.lora_adapter_path}")
            
            self.model.eval()
            self.is_loaded = True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            self.is_loaded = False
            raise
    
    def generate(self, 
                 instruction: str,
                 input_text: str,
                 max_new_tokens: int = 1024,
                 temperature: float = 0.7) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            instruction: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            input_text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„
        
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        if not self.is_loaded or self.model is None or self.tokenizer is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. _load_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        try:
            messages = [
                {"role": "system", "content": instruction},
                {"role": "user", "content": input_text}
            ]
            
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=0.9,
                    do_sample=True,
                    repetition_penalty=1.1
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return response
            
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return f"[ì˜¤ë¥˜] í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"
