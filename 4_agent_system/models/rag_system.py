"""
4_agent_system/models/rag_system.py
RAG ì‹œìŠ¤í…œ - ê³¼ê±° ì´ë ¥, ë§¤ë‰´ì–¼ ê²€ìƒ‰
"""

import os
from typing import List, Dict, Optional
import torch

try:
    from langchain.embeddings import HuggingFaceEmbeddings
    from langchain.vectorstores import FAISS
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.docstore.document import Document
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("âš ï¸  LangChain ë¯¸ì„¤ì¹˜: pip install langchain faiss-cpu sentence-transformers")

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    print("âš ï¸  ChromaDB ë¯¸ì„¤ì¹˜: pip install chromadb")


class RAGSystem:
    """RAG ì‹œìŠ¤í…œ - ê³¼ê±° ì´ë ¥, ë§¤ë‰´ì–¼ ê²€ìƒ‰"""
    
    def __init__(self, knowledge_base_path: str = None, use_chromadb: bool = False):
        """
        Args:
            knowledge_base_path: ì§€ì‹ ë² ì´ìŠ¤ ê²½ë¡œ (Noneì´ë©´ ìë™ íƒì§€)
            use_chromadb: ChromaDB ì‚¬ìš© ì—¬ë¶€ (Trueë©´ ChromaDB, Falseë©´ FAISS)
        """
        # ê²½ë¡œ ìë™ ì„¤ì •
        if knowledge_base_path is None:
            from ..utils.config import KNOWLEDGE_BASE_PATH, VECTOR_DB_PATH
            knowledge_base_path = str(KNOWLEDGE_BASE_PATH)
            self.vector_db_path = str(VECTOR_DB_PATH)
        else:
            self.vector_db_path = None
        
        self.knowledge_base_path = knowledge_base_path
        self.use_chromadb = use_chromadb and CHROMADB_AVAILABLE
        self.embeddings = None
        self.vectorstore = None
        self.chroma_collection = None
        self.documents = []
        self.is_loaded = False
        
        if not LANGCHAIN_AVAILABLE:
            print("âš ï¸  LangChain ì—†ìŒ. RAG ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ“š RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        self._initialize_embeddings()
        self._load_knowledge_base()
    
    def _initialize_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if not LANGCHAIN_AVAILABLE:
            return
        
        try:
            from ..utils.config import RAG_CONFIG
            embedding_model = RAG_CONFIG.get("embedding_model", "BAAI/bge-m3")
            
            self.embeddings = HuggingFaceEmbeddings(
                model_name=embedding_model,
                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
            )
            print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({embedding_model})")
        except Exception as e:
            print(f"âš ï¸  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # Fallback
            try:
                from ..utils.config import RAG_CONFIG
                fallback_model = RAG_CONFIG.get("fallback_embedding_model", 
                                                "sentence-transformers/all-MiniLM-L6-v2")
                self.embeddings = HuggingFaceEmbeddings(model_name=fallback_model)
                print(f"âœ… Fallback ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {fallback_model}")
            except Exception as e2:
                print(f"âŒ Fallback ì„ë² ë”© ëª¨ë¸ë„ ë¡œë“œ ì‹¤íŒ¨: {e2}")
                self.embeddings = None
    
    def _load_knowledge_base(self):
        """ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ë° ë²¡í„°í™”"""
        if not LANGCHAIN_AVAILABLE or self.embeddings is None:
            print("âš ï¸  LangChain ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ ì—†ìŒ. ìƒ˜í”Œ ë¬¸ì„œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self._load_sample_documents()
            return
        
        os.makedirs(self.knowledge_base_path, exist_ok=True)
        
        # ChromaDB ì‚¬ìš© ì‹œë„
        if self.use_chromadb and self.vector_db_path:
            try:
                self._load_from_chromadb()
                if self.is_loaded:
                    return
            except Exception as e:
                print(f"âš ï¸  ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("   FAISSë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        
        # íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
        try:
            self._load_from_files()
            if self.is_loaded:
                return
        except Exception as e:
            print(f"âš ï¸  íŒŒì¼ì—ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ìƒ˜í”Œ ë¬¸ì„œë¡œ í´ë°±
        print("âš ï¸  ì§€ì‹ ë² ì´ìŠ¤ íŒŒì¼ ì—†ìŒ. ìƒ˜í”Œ ë¬¸ì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        self._load_sample_documents()
    
    def _load_from_chromadb(self):
        """ChromaDBì—ì„œ ë¡œë“œ"""
        if not CHROMADB_AVAILABLE or not self.vector_db_path:
            return
        
        try:
            client = chromadb.PersistentClient(
                path=str(self.vector_db_path),
                settings=Settings(allow_reset=False)
            )
            collection = client.get_collection("manufacturing_kb")
            self.chroma_collection = collection
            self.is_loaded = True
            print(f"âœ… ChromaDBì—ì„œ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        except Exception:
            pass
    
    def _load_from_files(self):
        """íŒŒì¼ì—ì„œ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        from pathlib import Path
        kb_path = Path(self.knowledge_base_path)
        
        if not kb_path.exists():
            return
        
        # ì‹¤ì œ íŒŒì¼ ë¡œë“œ ë¡œì§ì€ setup_rag.py ì°¸ê³ 
        # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ìƒ˜í”Œë§Œ ì‚¬ìš©
        pass
    
    def _load_sample_documents(self):
        """ìƒ˜í”Œ ë¬¸ì„œ ë¡œë“œ (í´ë°±)"""
        if not LANGCHAIN_AVAILABLE or self.embeddings is None:
            return
        
        sample_docs = self._create_sample_documents()
        
        # ë¬¸ì„œ ë¶„í• 
        from ..utils.config import RAG_CONFIG
        chunk_size = RAG_CONFIG.get("chunk_size", 500)
        chunk_overlap = RAG_CONFIG.get("chunk_overlap", 50)
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        self.documents = []
        for doc_dict in sample_docs:
            doc = Document(
                page_content=doc_dict['content'],
                metadata=doc_dict['metadata']
            )
            self.documents.append(doc)
        
        splits = text_splitter.split_documents(self.documents)
        
        # Vector DB ìƒì„±
        if len(splits) > 0:
            self.vectorstore = FAISS.from_documents(splits, self.embeddings)
            self.is_loaded = True
            print(f"âœ… ìƒ˜í”Œ ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ ({len(splits)}ê°œ ì²­í¬)")
        else:
            print("âš ï¸  ì§€ì‹ ë² ì´ìŠ¤ ë¬¸ì„œ ì—†ìŒ")
    
    def _create_sample_documents(self) -> List[Dict]:
        """ìƒ˜í”Œ ì§€ì‹ ë² ì´ìŠ¤ ë¬¸ì„œ ìƒì„±"""
        return [
            {
                'content': """
                [ê³¼ê±° ì´ë ¥ #2023-08-15]
                ì„¤ë¹„: ì‚¬ì¶œê¸°-2í˜¸ê¸°
                ì¦ìƒ: ì‹¤ë¦°ë” ì˜¨ë„ ê¸‰ìƒìŠ¹ (235Â°C)
                ì›ì¸: íˆí„° ì½”ì¼ ë‹¨ì„ 
                ì¡°ì¹˜: íˆí„° êµì²´ í›„ ì •ìƒí™”
                ì†Œìš”ì‹œê°„: 4ì‹œê°„
                """,
                'metadata': {
                    'type': 'ê³¼ê±°_ì´ë ¥',
                    'equipment': 'ì‚¬ì¶œê¸°',
                    'anomaly': 'ì˜¨ë„_ì´ìƒ'
                }
            },
            {
                'content': """
                [ì„¤ë¹„ ë§¤ë‰´ì–¼ 3.2ì ˆ - ì˜¨ë„ ê´€ë¦¬]
                ì‹¤ë¦°ë” ì˜¨ë„ê°€ ì„¤ì •ê°’ Â±15Â°Cë¥¼ ë²—ì–´ë‚  ê²½ìš°:
                1. íˆí„° ì €í•­ê°’ ì¸¡ì • (ì •ìƒ: 30~35Î©)
                2. ì—´ì „ëŒ€ ì„¼ì„œ ì ê²€
                3. ì˜¨ë„ ì œì–´ê¸° íŒŒë¼ë¯¸í„° í™•ì¸
                ê¸´ê¸‰ ì¡°ì¹˜: ì¦‰ì‹œ ì„¤ë¹„ ì •ì§€
                """,
                'metadata': {
                    'type': 'ë§¤ë‰´ì–¼',
                    'equipment': 'ì‚¬ì¶œê¸°',
                    'section': 'ì˜¨ë„_ê´€ë¦¬'
                }
            },
            {
                'content': """
                [Trouble Shooting Guide]
                ì••ë ¥ ì´ìƒ ë°œìƒ ì‹œ ì ê²€ ìˆœì„œ:
                1. ìœ ì••íŒí”„ ì••ë ¥ ê²Œì´ì§€ í™•ì¸
                2. ì‹¤ë¦°ë” ì”° ëˆ„ìœ  ì ê²€
                3. ë°°ê´€ ì—°ê²°ë¶€ ì ê²€
                4. ì••ë ¥ ì„¼ì„œ êµì •
                ì£¼ì˜: ì••ë ¥ì´ ì •ìƒ ë²”ìœ„ì˜ 70% ì´í•˜ë©´ ì¦‰ì‹œ ì •ì§€
                """,
                'metadata': {
                    'type': 'Trouble_Shooting',
                    'equipment': 'ì „ì²´',
                    'anomaly': 'ì••ë ¥_ì´ìƒ'
                }
            },
            {
                'content': """
                [ì •ë¹„ ì´ë ¥ DB]
                ìµœê·¼ 6ê°œì›” ì§„ë™ ì´ìƒ ì¼€ì´ìŠ¤:
                - ë² ì–´ë§ ë§ˆëª¨: 5ê±´ (í‰ê·  ë³µêµ¬ ì‹œê°„ 6ì‹œê°„)
                - êµ¬ë™ë¶€ ì–¸ë°¸ëŸ°ìŠ¤: 3ê±´ (í‰ê·  ë³µêµ¬ ì‹œê°„ 4ì‹œê°„)
                - ì²´ê²°ë¶€ í’€ë¦¼: 2ê±´ (í‰ê·  ë³µêµ¬ ì‹œê°„ 2ì‹œê°„)
                ì˜ˆë°© ì¡°ì¹˜: ì›” 1íšŒ ì§„ë™ ì¸¡ì • ë° ë² ì–´ë§ ê·¸ë¦¬ìŠ¤ ì£¼ì…
                """,
                'metadata': {
                    'type': 'ì •ë¹„_ì´ë ¥',
                    'anomaly': 'ì§„ë™_ì´ìƒ',
                    'period': 'ìµœê·¼_6ê°œì›”'
                }
            },
            {
                'content': """
                [8D Report ì˜ˆì‹œ #2024-03-20]
                D1: íŒ€ êµ¬ì„± - ìƒì‚°ê¸°ìˆ íŒ€, í’ˆì§ˆíŒ€
                D2: ë¬¸ì œ ì •ì˜ - ì‚¬ì¶œê¸° 3í˜¸ê¸° ì˜¨ë„ ì´ìƒ
                D3: ì„ì‹œ ì¡°ì¹˜ - ì„¤ë¹„ ì •ì§€, ìƒì‚°í’ˆ ê²©ë¦¬
                D4: ê·¼ë³¸ ì›ì¸ - ëƒ‰ê°ìˆ˜ ìˆœí™˜ íŒí”„ ê³ ì¥
                D5: ì˜êµ¬ ëŒ€ì±… - íŒí”„ êµì²´, ì˜ˆë¹„í’ˆ í™•ë³´
                D6: ì‹¤í–‰ ë° ê²€ì¦ - 48ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì •ìƒ
                D7: ì¬ë°œ ë°©ì§€ - PM ì£¼ê¸° ì¡°ì •, ì„¼ì„œ ì¶”ê°€
                """,
                'metadata': {
                    'type': '8D_Report',
                    'equipment': 'ì‚¬ì¶œê¸°',
                    'date': '2024-03-20'
                }
            }
        ]
    
    def search(self, query: str, k: int = None) -> List[Dict]:
        """
        ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (Noneì´ë©´ ì„¤ì •ê°’ ì‚¬ìš©)
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if k is None:
            from ..utils.config import RAG_CONFIG
            k = RAG_CONFIG.get("search_k", 3)
        
        # ChromaDB ì‚¬ìš©
        if self.use_chromadb and self.chroma_collection is not None:
            try:
                results = self.chroma_collection.query(
                    query_texts=[query],
                    n_results=k
                )
                
                retrieved = []
                if results['documents'] and len(results['documents'][0]) > 0:
                    for i, (doc, metadata) in enumerate(zip(
                        results['documents'][0],
                        results['metadatas'][0] if results['metadatas'] else [{}] * len(results['documents'][0])
                    )):
                        retrieved.append({
                            'content': doc,
                            'metadata': metadata,
                            'similarity': 0.9 - (i * 0.1)  # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ì¶”ì •
                        })
                return retrieved
            except Exception as e:
                print(f"âš ï¸  ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # FAISS ì‚¬ìš©
        if self.vectorstore is not None:
            try:
                results = self.vectorstore.similarity_search_with_score(query, k=k)
                
                retrieved = []
                for doc, score in results:
                    retrieved.append({
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'similarity': float(1 - score) if score <= 1.0 else float(1 / (1 + score))  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    })
                
                return retrieved
            except Exception as e:
                print(f"âš ï¸  FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        print("âš ï¸  ë²¡í„° ìŠ¤í† ì–´ ì—†ìŒ. ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return []